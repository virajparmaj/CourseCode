{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fece59f05ca94700b12584b792fb1884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03ac721fd9854e179b7b2593d1371f4b",
              "IPY_MODEL_2828d6a05f56447f98a0d2a5e5082419",
              "IPY_MODEL_12f10d727bf2464dab88ff9d351a9061"
            ],
            "layout": "IPY_MODEL_d3cfe514080141e0ab081e0a8b51f2ad"
          }
        },
        "03ac721fd9854e179b7b2593d1371f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_769ef74d6d1947cebc89f62c3efd17e2",
            "placeholder": "​",
            "style": "IPY_MODEL_cdded7e7a4594898a45c70bc131e509e",
            "value": "README.md: 100%"
          }
        },
        "2828d6a05f56447f98a0d2a5e5082419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cceb4459a3c45599839560057769a2b",
            "max": 38515,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b82ef29260034d34baefc181665f91d7",
            "value": 38515
          }
        },
        "12f10d727bf2464dab88ff9d351a9061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d75fc8df22414973802c682c8463f68d",
            "placeholder": "​",
            "style": "IPY_MODEL_7ce9988ff8e545dba2f1d29ceb68c6bb",
            "value": " 38.5k/38.5k [00:00&lt;00:00, 433kB/s]"
          }
        },
        "d3cfe514080141e0ab081e0a8b51f2ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "769ef74d6d1947cebc89f62c3efd17e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdded7e7a4594898a45c70bc131e509e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cceb4459a3c45599839560057769a2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b82ef29260034d34baefc181665f91d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d75fc8df22414973802c682c8463f68d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ce9988ff8e545dba2f1d29ceb68c6bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32695432e4eb4301922c12706392b903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f29d7fada8364a68b758ee7faa73a95a",
              "IPY_MODEL_12d9ecf6fd0846c7bfc59005e939f429",
              "IPY_MODEL_21a5d5904a6b46aca13ad74e6f7285b4"
            ],
            "layout": "IPY_MODEL_b6052c4b98e14261833bf102c950f48e"
          }
        },
        "f29d7fada8364a68b758ee7faa73a95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fbd62f475ab4ab8b2dc54c88770592f",
            "placeholder": "​",
            "style": "IPY_MODEL_d1111a605597489e9358f02b05052bae",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "12d9ecf6fd0846c7bfc59005e939f429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ae2bb0c72734e8ab64d35610ccab796",
            "max": 18373460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f69c9e1c7edb49c79edc46c96855b5f6",
            "value": 18373460
          }
        },
        "21a5d5904a6b46aca13ad74e6f7285b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5acd2288bf48439a871f6c4eb893bfa2",
            "placeholder": "​",
            "style": "IPY_MODEL_577af7c5fbe84da9a61aee25d6340374",
            "value": " 18.4M/18.4M [00:00&lt;00:00, 30.5MB/s]"
          }
        },
        "b6052c4b98e14261833bf102c950f48e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fbd62f475ab4ab8b2dc54c88770592f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1111a605597489e9358f02b05052bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ae2bb0c72734e8ab64d35610ccab796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f69c9e1c7edb49c79edc46c96855b5f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5acd2288bf48439a871f6c4eb893bfa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "577af7c5fbe84da9a61aee25d6340374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c47b1c5da89444c697f890848cf02c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dfc18886ec9341439f7f47ea56359b41",
              "IPY_MODEL_a4a16cc62ed849f283da415b90ae08eb",
              "IPY_MODEL_67546444ec07438f8b04f4e12bc5f919"
            ],
            "layout": "IPY_MODEL_512cc61272bb452bb38f2107329f4f7b"
          }
        },
        "dfc18886ec9341439f7f47ea56359b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22907d8a707f4a2f80e5c1f77a53b7e9",
            "placeholder": "​",
            "style": "IPY_MODEL_df70277f919c4b70b5b22cb10df63b98",
            "value": "Generating train split: 100%"
          }
        },
        "a4a16cc62ed849f283da415b90ae08eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_713bc3595e674c14a3b99138aa471b04",
            "max": 9884,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_134436ba087a4bb1a61b94b04b53b948",
            "value": 9884
          }
        },
        "67546444ec07438f8b04f4e12bc5f919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb6b677d6dc4a2c98e9c5d2c6833b2e",
            "placeholder": "​",
            "style": "IPY_MODEL_a8ddd71bd71e4c43acdd7b2751fbbae4",
            "value": " 9884/9884 [00:00&lt;00:00, 17832.55 examples/s]"
          }
        },
        "512cc61272bb452bb38f2107329f4f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22907d8a707f4a2f80e5c1f77a53b7e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df70277f919c4b70b5b22cb10df63b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "713bc3595e674c14a3b99138aa471b04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "134436ba087a4bb1a61b94b04b53b948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cb6b677d6dc4a2c98e9c5d2c6833b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8ddd71bd71e4c43acdd7b2751fbbae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fce0425160542619b62126e383b15f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57f929e9cc084e65ae69841b1bab331b",
              "IPY_MODEL_524b8d84ea1643dca3bf03d55b4c2326",
              "IPY_MODEL_d3b2a41052bd4363a738f6e5e961c688"
            ],
            "layout": "IPY_MODEL_6037111619b14f9ebcfc25beaf684e02"
          }
        },
        "57f929e9cc084e65ae69841b1bab331b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d91fb02701954eac901f1301fd133eff",
            "placeholder": "​",
            "style": "IPY_MODEL_176007c44d3e457fa79a7b3eabfce0d0",
            "value": "100%"
          }
        },
        "524b8d84ea1643dca3bf03d55b4c2326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0201e6f0ea5f4b1580e0372788170eb3",
            "max": 603,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a6e39890f4648c48f7bdddd77236e6b",
            "value": 603
          }
        },
        "d3b2a41052bd4363a738f6e5e961c688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2224c23499934fa7854d90f261bfeb8c",
            "placeholder": "​",
            "style": "IPY_MODEL_d881403734d744e09322079bceb0a6f9",
            "value": " 603/603 [01:04&lt;00:00, 10.45it/s]"
          }
        },
        "6037111619b14f9ebcfc25beaf684e02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d91fb02701954eac901f1301fd133eff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "176007c44d3e457fa79a7b3eabfce0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0201e6f0ea5f4b1580e0372788170eb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6e39890f4648c48f7bdddd77236e6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2224c23499934fa7854d90f261bfeb8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d881403734d744e09322079bceb0a6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL8T2iZUN2Qj"
      },
      "source": [
        "# CS 447 Homework 1 $-$ Tokenization & Language Models\n",
        "In this homework we will study **tokenization** and **language modeling**. In particular, you will build a WordPiece tokenizer and some n-gram language models on a corpus of Wikipedia articles.\n",
        "\n",
        "This notebook is designed to be run in Google Colab. Navigate to <TT>colab.research.google.com</TT> and upload this notebook. Then follow the instructions in the notebook to do the assignent.\n",
        "\n",
        "To run the notebook, you will need to connect to a Runtime. For this homework, all you need is a CPU. You can change the runtime by going to <TT>Runtime > Change runtime type</TT> and selecting <TT>None</TT> in the <TT>Hardware Accelerator</TT> field. We encourage you to disconnect from the runtime when you are not using it, as Google Colab can limit your resources if you overuse them. You can read more about Google Colab at https://research.google.com/colaboratory/faq.html.\n",
        "\n",
        "We have imported all the libraries you need to do this homework. <b>You should not import any extra libraries.</b> If you do, the autograder will fail to run your code.\n",
        "\n",
        "**Reminder: The course policy of this class prohibits the use of AI tools to help with coding, such as Chat-GPT or other code completion software. Further, you may not look at or copy from code repositories online; and while you may discuss the homework with your classmates, you may *not* share code with each other.** You are of course welcome to look at general Python materials (such as Python or Pytorch tutorials and documentation)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "6zXwUyLZogP_",
        "outputId": "af271679-c486-4c85-9c91-869556c0a2ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: WordPiece Tokenization [28 points]\n",
        "\n",
        "Here you will implement WordPiece tokenization (you can read more about WordPiece in this [paper](https://arxiv.org/pdf/1609.08144))."
      ],
      "metadata": {
        "id": "FCTcbDLsh-nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download & preprocess the data\n",
        "\n",
        "We use a subset of the FineWeb BBC News dataset. You can see the Huggingface dataset card [here](https://huggingface.co/datasets/permutans/fineweb-bbc-news) (including a discussion of potential limitations or biases), and the corresponding research paper [here](https://arxiv.org/pdf/2406.17557)."
      ],
      "metadata": {
        "id": "UU08lg2JoX9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### DO NOT EDIT ###\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class NewsDataset():\n",
        "    \"\"\"\n",
        "    A class to manage and preprocess the News dataset for this homework.\n",
        "\n",
        "    Attributes:\n",
        "        train (list[str]): The training dataset as a list of sentences.\n",
        "        test (list[str]): The testing dataset as a list of sentences.\n",
        "    \"\"\"\n",
        "    def __init__(self, redownload_dataset=False):\n",
        "        \"\"\"\n",
        "        Initializes the NewsDataset object. If the dataset files do not exist\n",
        "        or redownload_dataset is set to True, it downloads the dataset.\n",
        "\n",
        "        Args:\n",
        "            redownload_dataset (bool): If True, redownloads the dataset.\n",
        "        \"\"\"\n",
        "        self.train, self.test = [], []\n",
        "        self.load_data(redownload_dataset)\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Returns a string representation of the dataset, including the number of\n",
        "        training and testing examples.\n",
        "\n",
        "        Returns:\n",
        "            str: Information about the dataset size\n",
        "        \"\"\"\n",
        "        repr = {}\n",
        "        repr[\"train\"] = len(self.train) if self.train else 0\n",
        "        repr[\"test\"] = len(self.test) if self.test else 0\n",
        "        return f\"{type(self).__name__}({repr})\"\n",
        "\n",
        "    @staticmethod\n",
        "    def isValid(s: str) -> bool:\n",
        "        \"\"\"\n",
        "        Checks whether a given string is valid for inclusion in the dataset.\n",
        "        A string is considered valid if it contains more than five words and\n",
        "        at least one alphabetic character.\n",
        "\n",
        "        Args:\n",
        "            s (str): The string to validate.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the string is valid, False otherwise.\n",
        "        \"\"\"\n",
        "        return s and (len(s.strip().split()) > 5) and any(c.isalpha() for c in s)\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(paragraph: str):\n",
        "        paragraph = paragraph.translate(str.maketrans({'?': '.', '!': '.'}))\n",
        "        sentences = paragraph.split('.')\n",
        "        sentences = [s.replace(\"\\\"\", \"\").replace(\"\\'\", \"\").strip().replace(\"\\n\", \" \") + '.' for s in sentences]\n",
        "        sentences = [s.lower() for s in sentences if '  ' not in s]\n",
        "        sentences = [s for s in sentences if not any(char.isdigit() for char in s)]\n",
        "        return sentences\n",
        "\n",
        "    def preprocessWP(self, data):\n",
        "        \"\"\"\n",
        "        Preprocesses a dataset by tokenizing them into\n",
        "        sentences and filtering out invalid ones.\n",
        "\n",
        "        Args:\n",
        "            data: An iterable containing the text of articles.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: A list of valid sentences.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for item in data:\n",
        "            sentences = self.preprocess(item[\"text\"])\n",
        "            sentences = [sent for sent in sentences if self.isValid(sent)]\n",
        "            dataset += sentences\n",
        "        return dataset\n",
        "\n",
        "    def downloadDatasetWP(self):\n",
        "        \"\"\"\n",
        "        Downloads and preprocesses the News dataset.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: A list of preprocessed sentences from the dataset.\n",
        "        \"\"\"\n",
        "        ds = load_dataset(\"permutans/fineweb-bbc-news\", \"sample-10BT\")\n",
        "        ds = self.preprocessWP(ds[\"train\"])\n",
        "        return ds\n",
        "\n",
        "    def generateDatasetWP(self):\n",
        "        \"\"\"\n",
        "        Downloads, preprocesses, and splits the dataset into training and\n",
        "        testing sets.\n",
        "        The processed data is saved as parquet files.\n",
        "        \"\"\"\n",
        "        ds = self.downloadDatasetWP()\n",
        "        data = {\"text\": ds}\n",
        "        df = pd.DataFrame(data)\n",
        "        train = df.sample(frac=0.05,random_state=200)\n",
        "        test = train.sample(frac=0.1,random_state=200)\n",
        "        train = train.drop(test.index)\n",
        "        train.to_parquet('train.parquet', index=False)\n",
        "        test.to_parquet('test.parquet', index=False)\n",
        "\n",
        "    def load_data(self, redownload_dataset):\n",
        "        \"\"\"\n",
        "        Loads the dataset from parquet files. If the files do not exist or\n",
        "        redownload_dataset is set to True, it downloads the dataset.\n",
        "        \"\"\"\n",
        "        if (redownload_dataset or\n",
        "                not os.path.exists('train.parquet') or\n",
        "                not os.path.exists('test.parquet')):\n",
        "            self.generateDatasetWP()\n",
        "        self.train, self.test = (pd.read_parquet('train.parquet')[\"text\"].tolist(),\n",
        "                                 pd.read_parquet('test.parquet')[\"text\"].tolist())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset = NewsDataset()\n",
        "\n",
        "    print('\\n', dataset, '\\n\\nExample sentences:')\n",
        "\n",
        "    for sentence in dataset.train[:10]:\n",
        "        print('\\t', sentence)"
      ],
      "metadata": {
        "id": "LwpobI7Moeo-",
        "outputId": "834ac120-7f70-4963-e6a2-0e0b975b7636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "fece59f05ca94700b12584b792fb1884",
            "03ac721fd9854e179b7b2593d1371f4b",
            "2828d6a05f56447f98a0d2a5e5082419",
            "12f10d727bf2464dab88ff9d351a9061",
            "d3cfe514080141e0ab081e0a8b51f2ad",
            "769ef74d6d1947cebc89f62c3efd17e2",
            "cdded7e7a4594898a45c70bc131e509e",
            "0cceb4459a3c45599839560057769a2b",
            "b82ef29260034d34baefc181665f91d7",
            "d75fc8df22414973802c682c8463f68d",
            "7ce9988ff8e545dba2f1d29ceb68c6bb",
            "32695432e4eb4301922c12706392b903",
            "f29d7fada8364a68b758ee7faa73a95a",
            "12d9ecf6fd0846c7bfc59005e939f429",
            "21a5d5904a6b46aca13ad74e6f7285b4",
            "b6052c4b98e14261833bf102c950f48e",
            "9fbd62f475ab4ab8b2dc54c88770592f",
            "d1111a605597489e9358f02b05052bae",
            "6ae2bb0c72734e8ab64d35610ccab796",
            "f69c9e1c7edb49c79edc46c96855b5f6",
            "5acd2288bf48439a871f6c4eb893bfa2",
            "577af7c5fbe84da9a61aee25d6340374",
            "c47b1c5da89444c697f890848cf02c5d",
            "dfc18886ec9341439f7f47ea56359b41",
            "a4a16cc62ed849f283da415b90ae08eb",
            "67546444ec07438f8b04f4e12bc5f919",
            "512cc61272bb452bb38f2107329f4f7b",
            "22907d8a707f4a2f80e5c1f77a53b7e9",
            "df70277f919c4b70b5b22cb10df63b98",
            "713bc3595e674c14a3b99138aa471b04",
            "134436ba087a4bb1a61b94b04b53b948",
            "8cb6b677d6dc4a2c98e9c5d2c6833b2e",
            "a8ddd71bd71e4c43acdd7b2751fbbae4"
          ]
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/38.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fece59f05ca94700b12584b792fb1884"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/18.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32695432e4eb4301922c12706392b903"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/9884 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c47b1c5da89444c697f890848cf02c5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " NewsDataset({'train': 7948, 'test': 883}) \n",
            "\n",
            "Example sentences:\n",
            "\t the three are suspected of aggravated attempts to avoid tax controls, according to swedish prosecutor olof sahlgren.\n",
            "\t missing killer alan john giles seen in worcestershire village a convicted killer who absconded from prison has possibly been seen in worcestershire, police said.\n",
            "\t it will be a lot tighter than people are expecting, said the three-times league cup winner.\n",
            "\t they look cold, half of them.\n",
            "\t but it could take a decade to build a base for the submarines elsewhere in the uk, it said.\n",
            "\t hes a proven premier league performer who brings quality to our squad, boss gary megson told the club website.\n",
            "\t but the industry is still so new there are many different approaches.\n",
            "\t un chief ban ki-moon is sending two top aides to the country to help investigate the alleged assaults in the countrys volatile eastern region.\n",
            "\t cheng cheng ensured that his classmates shouted down xu xiaofei before she had even started to speak, and she found it difficult to recover.\n",
            "\t the report in beijings official english-language china daily notes that the country has suffered a series of knife attacks in schools and day-care centres in recent months.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bold text**## <font color='red'>TODO:</font> Tokenizer\n",
        "\n",
        "First, you will write a training (learning) algorithm, which takes in a corpus (list of sentences), and returns a token vocabulary. Then, you will write the tokenization algorithm. This takes in a sentence and returns its tokenization based on the vocabulary.\n",
        "\n",
        "Your code for this section will all be in the `WordPieceTokenizer` class, where you will implement several functions.\n",
        "\n",
        "### Training algorithm\n",
        "\n",
        "WordPiece learns a token vocabulary in an iterative fashion. Starting with an initial token vocabulary (the set of characters in the training data), each step merges a pair of consecutive tokens, where the pair of tokens is selected according to a scoring function (described below). The process is continued until a desired `vocab_size` is reached.\n",
        "\n",
        "You will complete this algorithm by implementing several functions in the `WordPieceTokenizer` class:\n",
        "\n",
        "1. **`initialize(self, train_corpus)`: [6 points]** This returns the initial vocabulary, the initial tokenization of each word, the word frequncies, and a mapping from each token to the words containing that token. First, split each sentence in `train_corpus` on the space character. The initial vocabulary is the set of all characters that occur in any word; but characters occuring in the middle of a word should be prepended with the special sequence `##`. For example, if your corpus is the sentence `the old man the boat.`, your initial vocabulary would be:\n",
        "<div align=\"center\"><code>{'t', '##h', '##e', 'o', '##l', '##d', 'm', '##a', '##n', 'b', '##o', '##t', '##.'}</code></div>\n",
        "This function should also compute the initial tokenization of each word, storing them in a dictionary from word to tokenization. For example:\n",
        "<div align=\"center\"><code>{'the': ('t', '##h', '##e;), 'old': ('o', '##l', '##d'),                \n",
        "    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}\n",
        "</code></div>\n",
        "The function should also count the frequency of each word in the training corpus, and return a mapping from token to the words that use that token. For example:\n",
        "<div align=\"center\"><code>word_freqs = {'the': 100, 'old': 34, 'man': 76, 'boat.': 18}</code></div>\n",
        "    <div align=\"center\"><code>tokens2word = {'t': {'the'}, '##h': {'the'}, '##e': {'the'}, ...}</code></div>\n",
        "\n",
        "2. **`find_best_pair(self, word_tokenizations, word_freqs, vocab)`: [6 points]** This function computes a score for each *consecutive pair* $(t_1, t_2)$ of tokens, and returns the pair of tokens with highest score. The scoring function is $$\\frac{c(t_1, t_2) \\cdot |V|}{c(t_1)\\cdot c(t_2)}$$where $c(t_1, t_2)$ is the number of times $t_1$ and $t_2$ occur consecutively in the corpus, $c(t_i)$ is the nubmer of times token $t_i$ occurs, and $|V|$ is the size of the current token vocabulary. The function will return the pair with highest score (ties broken alphabetically).\n",
        "\n",
        "    The idea behind the scoring function is that a pair of tokens will have high score if they occur frequently together *and* each token does not occur frequently on its own. Factoring in the size of the vocabulary gives a (slight) preference to longer subword units as the vocabulary size grows.\n",
        "    \n",
        "\n",
        "3. **`merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word)`: [6 points]** This function updates your current vocabulary (`vocab`), tokenizations (`word_tokenizations`), and token-to-word mapping (`tokens2word`) based on the pair of tokens to merge (`best_pair`). The `vocab` should simply be updated to include the new token, and any word in `word_tokenizations` that contains the consecutive pair of tokens should be updated to merge that pair. For example, suppose the pair of tokens to merge is `(##h, ##e)` and `word_tokenizations` is\n",
        "<div align=\"center\"><code>  {'the': ('t', '##h', '##e;), 'old': ('o', '##l', '##d'),                \n",
        "    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}</code></div>\n",
        "Then <code>word_tokenizations</code> should be updated to\n",
        "<div align=\"center\"><code>  {'the': ('t', '##he'), 'old': ('o', '##l', '##d'),                      \n",
        "    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}</code></div>\n",
        "Similarly, <code>tokens2word</code> should be updated to include the new token:\n",
        "<div align=\"center\"><code>tokens2word['##he'] = {'the'}</code></div>\n",
        "<i>Note:</i> In order to pass the autograder, you should <i>not</i> remove <code>'the'</code> from <code>tokens2word['##h']</code> or <code>tokens2word['##e']</code>, despite the fact that these tokens are no longer used for that word.\n",
        "\n",
        "We provide you the function `train(self, train_corpus, vocab_size)` which iteratively calls these functions until the desired `vocab_size` is hit. You should NOT modify this function, but you should look at it to understand how it works.\n",
        "\n",
        "### Tokenization algorithm\n",
        "\n",
        "Finally, you will write the `tokenize` and `detokenize` functions, which use the token vocabulary you learned to tokenize and detokenize arbitrary sentences:\n",
        "\n",
        "1. **`tokenize(self, sentence)`: [5 points]** This function takes a sentence and returns its tokenization. First, split on the space character. For each word, find the longest substring starting at the beginning of the word that is in the token vocabulary, and choose that token. Then, iteratively repeat this procedure on the remainder of the word, until the entire word is tokenized. For example, consider the word `swiftly`:\n",
        "    * If `s` and `sw` are tokens but `swi` is not, then choose `sw`.\n",
        "    * Then, proceed with `##iftly`. If `##i`, `##if`, and `##ift` are tokens, but `##iftl` is not, choose `##ift`.\n",
        "    * Then, proceed with `##ly`. If both `##l` and `##ly` are tokens, choose `##ly`.\n",
        "\n",
        "  After tokenizing each word in this way, return a list containing the tokenization of each word separated by space tokens. For the sentence `he is swift.`, for example, you might return `['he', ' ', 'is', ' ', 'sw', '##ift', '##.']`\n",
        "\n",
        "  **Note:** If at any step during the tokenization of a word, you cannot find a matching token in your vocabulary, then the *entire* word should be tokenized as `<UNK>`. For example, if you are considering `##mat` but `##m`, `##ma`, and `##mat` are not in the vocabulary, the entire word (including the part of the word preceeding `##mat`) should be tokenized as `<UNK>`.\n",
        "\n",
        "\n",
        "2. **`detokenize(self, tokens)`: [5 points]** This takes a list of tokens and returns its corresponding sentence. Simply copy tokens into a string from left to right, being careful to remove `##` at the beginning of tokens."
      ],
      "metadata": {
        "id": "NqinUgNann-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordPieceTokenizer(object):\n",
        "    def __init__(self, train_corpus, vocab_size, do_print=False, do_tqdm=True):\n",
        "        '''\n",
        "        ### DO NOT EDIT ###\n",
        "        Trains a WordPiece tokenizer.\n",
        "        Args:\n",
        "            train_corpus: List of strings (each string is a sentence)\n",
        "            vocab_size: Integer indicating how big the vocabulary should be\n",
        "            do_print: Whether or not to print merges\n",
        "        '''\n",
        "        self.train_corpus = train_corpus\n",
        "        self.vocab_size = vocab_size\n",
        "        self.do_print = do_print\n",
        "        self.do_tqdm = do_tqdm\n",
        "        # Check that no sentence already contains the continuation marker \"##\"\n",
        "        for sent in train_corpus:\n",
        "            assert '##' not in sent, sent\n",
        "        self.vocab = None\n",
        "\n",
        "    def initialize(self, train_corpus):\n",
        "        '''\n",
        "        Initialize token vocabulary, word frequencies, and token-to-word dictionaries.\n",
        "        Args:\n",
        "            train_corpus: List of strings, where each string is a sentence\n",
        "        Returns:\n",
        "            vocab: A set containing the initial token vocabulary (each token is a string)\n",
        "            word_tokenizations: A dictionary of word to its tokenization (a tuple of strings)\n",
        "            word_freqs: A dictionary that maps each word to its frequency in the training data\n",
        "            tokens2word: A dictionary that maps each token to the set of words with that token\n",
        "        '''\n",
        "        vocab = {'<UNK>', ' '}  # Initialize vocab with special tokens (don't change this)\n",
        "        word_tokenizations, word_freqs, tokens2word = {}, {}, {}\n",
        "\n",
        "        for sentence in train_corpus:\n",
        "            words = sentence.split()\n",
        "            for word in words:\n",
        "                # Increase frequency count\n",
        "                word_freqs[word] = word_freqs.get(word, 0) + 1\n",
        "                # Create initial tokenization if not already done.\n",
        "                if word not in word_tokenizations:\n",
        "                    tokens = tuple([word[0]] + [\"##\" + ch for ch in word[1:]])\n",
        "                    word_tokenizations[word] = tokens\n",
        "                # Add tokens to vocab and update tokens2word mapping.\n",
        "                for token in word_tokenizations[word]:\n",
        "                    vocab.add(token)\n",
        "                    if token not in tokens2word:\n",
        "                        tokens2word[token] = set()\n",
        "                    tokens2word[token].add(word)\n",
        "        return vocab, word_tokenizations, word_freqs, tokens2word\n",
        "\n",
        "    def find_best_pair(self, word_tokenizations, word_freqs, vocab):\n",
        "        '''\n",
        "        Score all pairs of consecutive tokens (bigrams) in train_corpus, and return the pair with\n",
        "        highest score. If there is a tie, choose the pair that is first alphabetically.\n",
        "        Args:\n",
        "            word_freqs: Dictionary of word to frequency\n",
        "            word_tokenizations: Dictionary of word to its tokenization (a tuple of strings)\n",
        "            vocab: A set of tokens (strings)\n",
        "        Returns:\n",
        "            best_pair: The pair (tuple) of tokens (t1, t2) that has highest score\n",
        "            scores: Dictionary mapping each token pair to its score\n",
        "        '''\n",
        "        pair_counts = {}\n",
        "        token_counts = {}\n",
        "        # Count token occurrences and consecutive token pair occurrences (weighted by word frequency)\n",
        "        for word, tokens in word_tokenizations.items():\n",
        "            freq = word_freqs[word]\n",
        "            for token in tokens:\n",
        "                token_counts[token] = token_counts.get(token, 0) + freq\n",
        "            for i in range(len(tokens) - 1):\n",
        "                pair = (tokens[i], tokens[i+1])\n",
        "                pair_counts[pair] = pair_counts.get(pair, 0) + freq\n",
        "\n",
        "        scores = {}\n",
        "        best_pair = None\n",
        "        best_score = -1\n",
        "        V_size = len(vocab)\n",
        "        # Compute score for each pair: (c(t1, t2) * |V|) / (c(t1)*c(t2))\n",
        "        for pair, count in pair_counts.items():\n",
        "            t1, t2 = pair\n",
        "            score = (count * V_size) / (token_counts[t1] * token_counts[t2])\n",
        "            scores[pair] = score\n",
        "            if score > best_score or (score == best_score and (best_pair is None or pair < best_pair)):\n",
        "                best_score = score\n",
        "                best_pair = pair\n",
        "\n",
        "        return best_pair, scores\n",
        "\n",
        "    def merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word):\n",
        "            '''\n",
        "            Update vocab, word_tokenizations, and tokens2word based on pair to merge.\n",
        "            Args:\n",
        "                best_pair: The pair of tokens that had highest score; that is, the pair to be merged\n",
        "                vocab: The token vocabulary (set of strings)\n",
        "                word_tokenizations: Dictionary from word to tokenization\n",
        "                tokens2word: A dictionary from token to the set of words with that token\n",
        "            Returns: Nothing\n",
        "            Modifies:\n",
        "                vocab: The new token is added to the vocab set.\n",
        "                word_tokenizations: For any word that contains the consecutive pair in best_pair,\n",
        "                  its tokenization is updated by merging that pair.\n",
        "                tokens2word: The new token is mapped to the set of words that use it.\n",
        "                          (Do NOT remove the original tokens from tokens2word.)\n",
        "            Hint: When looking for words that contain the new token, consider only words that use either token in best_pair.\n",
        "            '''\n",
        "            t1, t2 = best_pair\n",
        "            # When merging, remove the extra '##' from t2 if present.\n",
        "            if t2.startswith(\"##\"):\n",
        "                new_token = t1 + t2[2:]\n",
        "            else:\n",
        "                new_token = t1 + t2\n",
        "            vocab.add(new_token)\n",
        "            # Only examine words that contain either t1 or t2.\n",
        "            candidate_words = tokens2word.get(t1, set()).union(tokens2word.get(t2, set()))\n",
        "            for word in candidate_words:\n",
        "                tokens = list(word_tokenizations[word])\n",
        "                new_tokens = []\n",
        "                merged = False\n",
        "                i = 0\n",
        "                while i < len(tokens):\n",
        "                    if i < len(tokens) - 1 and tokens[i] == t1 and tokens[i+1] == t2:\n",
        "                        new_tokens.append(new_token)\n",
        "                        merged = True\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        new_tokens.append(tokens[i])\n",
        "                        i += 1\n",
        "                if merged:\n",
        "                    word_tokenizations[word] = tuple(new_tokens)\n",
        "                    if new_token not in tokens2word:\n",
        "                        tokens2word[new_token] = set()\n",
        "                    tokens2word[new_token].add(word)\n",
        "            # End of merge_best_pair\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        '''\n",
        "        ### DO NOT EDIT ###\n",
        "        Trains the WordPiece tokenization algorithm by iteratively merging tokens until\n",
        "        vocab_size is reached.\n",
        "        Args:\n",
        "            train_corpus: List of strings, where each string is a sentence\n",
        "            vocab_size: The desired vocabulary size\n",
        "        Returns:\n",
        "            vocab: The set of tokens in the vocabulary, returned as a sorted list\n",
        "        '''\n",
        "        vocab, word_tokenizations, word_freqs, tokens2word = self.initialize(self.train_corpus)\n",
        "        init_vocab_size = len(vocab)\n",
        "        if self.do_print:\n",
        "            print(\"Initial vocab size:\", init_vocab_size)\n",
        "        assert self.vocab_size >= len(vocab), 'Cannot give a vocab size smaller than initial vocab size'\n",
        "\n",
        "        itr = tqdm(range(self.vocab_size - len(vocab))) if self.do_tqdm else range(self.vocab_size - len(vocab))\n",
        "        for i in itr:\n",
        "            best_pair, scores = self.find_best_pair(word_tokenizations, word_freqs, vocab)\n",
        "            self.merge_best_pair(best_pair, vocab, word_tokenizations, tokens2word)\n",
        "            # Uncomment the next line if you wish to print each merge:\n",
        "            # print(\"\\tMerging \", best_pair)\n",
        "            assert len(vocab) == init_vocab_size + i + 1, str(len(vocab)) + ' ' + str(init_vocab_size) + ' ' + str(i)\n",
        "            if all(len(word_tokenizations[w]) == 1 for w in word_tokenizations):\n",
        "                print(\"All words have been maximally merged at vocab_size=\" + str(len(vocab)) + \" – breaking.\")\n",
        "                break\n",
        "        if self.do_print:\n",
        "            print(\"Done!\")\n",
        "        self.vocab = sorted(vocab)\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        '''\n",
        "        Tokenizes a sentence.\n",
        "        Args:\n",
        "            sentence: A string representing a sentence\n",
        "        Returns:\n",
        "            tokens: A list containing the tokens of the sentence\n",
        "        '''\n",
        "        assert type(sentence) == str\n",
        "        tokens = []\n",
        "        vocab_set = set(self.vocab)\n",
        "        words = sentence.split()\n",
        "        for idx, word in enumerate(words):\n",
        "            i = 0\n",
        "            word_tokens = []\n",
        "            while i < len(word):\n",
        "                found = False\n",
        "                # Greedily match the longest substring from position i.\n",
        "                for j in range(len(word), i, -1):\n",
        "                    candidate = word[i:j] if i == 0 else \"##\" + word[i:j]\n",
        "                    if candidate in vocab_set:\n",
        "                        word_tokens.append(candidate)\n",
        "                        i = j\n",
        "                        found = True\n",
        "                        break\n",
        "                if not found:\n",
        "                    # If no token is found at any step, mark the entire word as unknown.\n",
        "                    word_tokens = [\"<UNK>\"]\n",
        "                    break\n",
        "            tokens.extend(word_tokens)\n",
        "            if idx < len(words) - 1:\n",
        "                tokens.append(\" \")\n",
        "        return tokens\n",
        "\n",
        "    def detokenize(self, tokens):\n",
        "        '''\n",
        "        Detokenizes a sentence.\n",
        "        Args:\n",
        "            tokens: A list of tokens representing a sentence\n",
        "        Returns:\n",
        "            sentence: A string representing the sentence\n",
        "        '''\n",
        "        assert type(tokens) == list and len(tokens) > 0 and type(tokens[0]) == str\n",
        "        sentence = \"\"\n",
        "        for token in tokens:\n",
        "            if token == \" \":\n",
        "                sentence += token\n",
        "            elif token.startswith(\"##\"):\n",
        "                # Remove the \"##\" and append without a preceding space.\n",
        "                sentence += token[2:]\n",
        "            else:\n",
        "                # If token is a full word and not preceded by a space, add a space.\n",
        "                if sentence and not sentence.endswith(\" \"):\n",
        "                    sentence += \" \"\n",
        "                sentence += token\n",
        "        return sentence"
      ],
      "metadata": {
        "id": "RWRZljnLnMWu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sanity Check: Tokenizer Class\n",
        "\n",
        "The code below runs a sanity check for your `WordPieceTokenizer` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."
      ],
      "metadata": {
        "id": "nXCkjvBaLp_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sanity Check Code\n",
        "### DO NOT EDIT ###\n",
        "\n",
        "def check_dictionary(test_dict, correct_dict):\n",
        "    if not isinstance(test_dict, dict) or not isinstance(correct_dict, dict):\n",
        "        return \"INCORRECT\", \"Is not a dictionary.\"\n",
        "    if set(test_dict.keys()) != set(correct_dict.keys()):\n",
        "        return \"INCORRECT\", f\"Key mismatch: Missing {set(correct_dict.keys()) - set(test_dict.keys())}, Extra {set(test_dict.keys()) - set(correct_dict.keys())}\"\n",
        "    for key in correct_dict:\n",
        "        if test_dict[key] != correct_dict[key]:\n",
        "            return \"INCORRECT\", f\"Value mismatch for key '{key}': Expected {correct_dict[key]}, Got {test_dict[key]}\"\n",
        "    return \"CORRECT\", ''\n",
        "\n",
        "def check_set(test_set, correct_set):\n",
        "    if not isinstance(test_set, set) or not isinstance(correct_set, set):\n",
        "        return \"INCORRECT\", \"Is not a set.\"\n",
        "    if test_set != correct_set:\n",
        "        return \"INCORRECT\", f\"Set mismatch: Missing {correct_set - test_set}, Extra {test_set - correct_set}\"\n",
        "    return \"CORRECT\", \"\"\n",
        "\n",
        "def sanityCheckInitialize(train):\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "\n",
        "    print('\\n\\n--- TEST: initialize(self, train_corpus) ---')\n",
        "    res = tokenizer.initialize(train)\n",
        "    if len(res) != 4:\n",
        "        print('FAILED\\ninitialize(self, train_corpus) must return 4 items.')\n",
        "        return\n",
        "    vocab, word_tokenizations, word_freqs, tokens2word = res\n",
        "\n",
        "    correct_vocab = {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}\n",
        "    correct_word_tokenizations = {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\n",
        "    correct_word_freqs = {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\n",
        "    correct_tokens2word = {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'that', 'banana.', 'boat.', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'that', 'boat.', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'lied.', 'like'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}\n",
        "\n",
        "    pass1, msg1 = check_set(vocab, correct_vocab)\n",
        "    pass2, msg2 = check_dictionary(word_tokenizations, correct_word_tokenizations)\n",
        "    pass3, msg3 = check_dictionary(word_freqs, correct_word_freqs)\n",
        "    pass4, msg4 = check_dictionary(tokens2word, correct_tokens2word)\n",
        "\n",
        "    print('\\tvocab:\\t\\t\\t'+pass1+'\\t'+msg1 + '\\tYour vocab: ' +str(vocab) + '\\tCorrect vocab:' + str(correct_vocab))\n",
        "    print('\\tword_tokenizations:\\t'+pass2+'\\t'+msg2 + '\\tYour word_tokenizations: ' +str(word_tokenizations) + '\\tCorrect word_tokenizations: ' + str(correct_word_tokenizations))\n",
        "    print('\\tword_freqs:\\t\\t'+pass3+'\\t'+msg3 + '\\tYour word_freqs: ' +str(word_freqs) + '\\tCorrect word_freqs: ' + str(correct_word_freqs))\n",
        "    print('\\ttokens2word:\\t\\t'+pass4+'\\t'+msg4 + '\\tYour tokens2word: ' +str(tokens2word) + '\\tCorrect tokens2word: ' + str(correct_tokens2word))\n",
        "\n",
        "    if len({pass1, pass2, pass3, pass4}) == 1 and pass1 == 'CORRECT':\n",
        "        print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "def sanityCheckFindBestPair(train):\n",
        "    print('\\n\\n--- TEST: find_best_pair(self, word_tokenizations, word_freqs, vocab) ---') # max_len does not matter for this test\n",
        "    test_cases = ({'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}, [(({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##w', '##;'), {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##o', '##w;'), {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##r', '##ow;'), {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##r', '##row;'), {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##r', '##u'), {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('a', '##rrow;'), {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('f', '##ru'), {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('b', '##o'), {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##o', '##l'), {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##ol', '##d'), {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('f', '##l'), {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('o', '##l'), {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667})), (({'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('ol', '##d'), {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##d', '##.'), {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('k', '##n'), {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}), (('s', '##h'), {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}))])\n",
        "\n",
        "    word_freqs, test_cases = test_cases[0], test_cases[1]\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "\n",
        "    overall_pass = True\n",
        "    for i in range(len(test_cases)):\n",
        "        res = tokenizer.find_best_pair(test_cases[i][0][0], word_freqs, test_cases[i][0][1])\n",
        "        if len(res) != 2:\n",
        "            pass1, msg1 = 'INCORRECT', 'Did not return 2 items.'\n",
        "        else:\n",
        "            best_pair, scores = res\n",
        "            bp = best_pair == test_cases[i][1][0]\n",
        "            sc, scc = check_dictionary(scores, test_cases[i][1][1])\n",
        "            if bp and sc == 'CORRECT':\n",
        "                pass1, msg1 = 'CORRECT', ''\n",
        "            else:\n",
        "                overall_pass = False\n",
        "                pass1 = 'INCORRECT'\n",
        "                msg1 = ''\n",
        "                if not bp: msg1 += 'best_pair is incorrect. '\n",
        "                if sc == 'INCORRECT': msg1 += 'scores is incorrect (' + scc + ')'\n",
        "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tword_tokenizations: ' + str(test_cases[i][0][0]) + '\\tword_freqs: ' + str(word_freqs) + '\\tvocab: ' + str(test_cases[i][0][1]) +\n",
        "              '\\tYour best_pair: ' + str(best_pair) + '\\tCorrect best_pair: ' + str(test_cases[i][1][0]) + '\\tYour scores: ' + str(scores) + '\\tCorrect scores: ' + str(test_cases[i][1][1]))\n",
        "    if overall_pass: print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "def sanityCheckMergeBestPair(train):\n",
        "    print('\\n\\n--- TEST: merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word) ---') # max_len does not matter for this test\n",
        "    test_cases = [((('##w', '##;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}})), ((('##o', '##w;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}})), ((('##r', '##ow;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}})), ((('##r', '##row;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}})), ((('##r', '##u'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}})), ((('a', '##rrow;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}})), ((('f', '##ru'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}})), ((('b', '##o'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}})), ((('##o', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}})), ((('##ol', '##d'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}})), ((('f', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}})), ((('o', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}})), ((('ol', '##d'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}})), ((('##d', '##.'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}})), ((('k', '##n'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}})), ((('s', '##h'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}), ({'old', '<UNK>', 'sh', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}))]\n",
        "\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "\n",
        "    overall_pass = True\n",
        "    for i in range(len(test_cases)):\n",
        "        from copy import deepcopy\n",
        "        vocab, word_tokenizations, tokens2word = deepcopy(test_cases[i][0][1]), deepcopy(test_cases[i][0][2]), deepcopy(test_cases[i][0][3])\n",
        "        vocabo, word_tokenizationso, tokens2wordo = deepcopy(vocab), deepcopy(word_tokenizations), deepcopy(tokens2word)\n",
        "        tokenizer.merge_best_pair(test_cases[i][0][0], vocab, word_tokenizations, tokens2word)\n",
        "\n",
        "        pass1, msg0 = check_set(vocab, test_cases[i][1][0])\n",
        "        pass2, msg2 = check_dictionary(word_tokenizations, test_cases[i][1][1])\n",
        "        pass3, msg3 = check_dictionary(tokens2word, test_cases[i][1][2])\n",
        "        if pass1 == 'CORRECT' and pass2 == 'CORRECT' and pass3 == 'CORRECT':\n",
        "            pass1, msg1 = 'CORRECT', ''\n",
        "        else:\n",
        "            overall_pass = False\n",
        "            pass1 = 'INCORRECT'\n",
        "            msg1 = ''\n",
        "            if pass1 != 'CORRECT': msg1 += 'vocab is incorrect. (' + msg0 + ') '\n",
        "            if pass2 != 'CORRECT': msg1 += 'word_tokenizations is incorrect. (' + msg2 + ') '\n",
        "            if pass3 != 'CORRECT': msg1 += 'tokens2word is incorrect. (' + msg3 + ') '\n",
        "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tbest_pair: ' + str(test_cases[i][0][0]) + '\\tvocab: ' + str(vocabo) + '\\tword_tokenizations: ' + str(word_tokenizationso) + '\\ttokens2word: ' + str(tokens2wordo) +\n",
        "              '\\tYour vocab: ' + str(vocab) + '\\tCorrect vocab: ' + str(test_cases[i][1][0]) + '\\tYour word_tokenizations: ' + str(word_tokenizations) + '\\tCorrect word_tokenizations: ' + str(test_cases[i][1][1])+\n",
        "              '\\tYour tokens2word: ' + str(tokens2word) + '\\tCorrect tokens2word: ' + str(test_cases[i][1][2]))\n",
        "    if overall_pass: print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "\n",
        "def sanityCheckTokenize(train, test):\n",
        "\n",
        "    print('\\n\\n--- TEST: tokenize(self, sentence) ---')\n",
        "\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "    tokenizer.vocab = [' ', '##.', '##;', '##a', '##d', '##d.', '##e', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##ol', '##old', '##ow;', '##r', '##row;', '##rrow;', '##ru', '##s', '##t', '##u', '##w', '##w;', '<UNK>', 'a', 'arrow;', 'b', 'bo', 'f', 'fl', 'fru', 'h', 'k', 'kn', 'l', 'm', 'o', 'ol', 'old', 's', 'sh', 't']\n",
        "    correct_ans = [['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>'], ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']]\n",
        "\n",
        "    overall_pass = True\n",
        "    for i in range(len(test)):\n",
        "        toks = tokenizer.tokenize(test[i])\n",
        "        if toks == correct_ans[i]: pass1, msg1 = 'CORRECT', ''\n",
        "        else:\n",
        "            pass1, msg1 = 'INCORRECT', 'Tokenization is incorrect.'\n",
        "            overall_pass = False\n",
        "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tsentence: ' + str(test[i]) +\n",
        "              '\\tYour tokenization: ' + str(toks) + '\\tCorrect tokenization: ' + str(correct_ans[i]))\n",
        "\n",
        "    if overall_pass: print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "def sanityCheckDetokenize(train):\n",
        "\n",
        "    print('\\n\\n--- TEST: detokenize(self, tokens) ---')\n",
        "\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "    tokenizer.vocab = [' ', '##.', '##;', '##a', '##d', '##d.', '##e', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##ol', '##old', '##ow;', '##r', '##row;', '##rrow;', '##ru', '##s', '##t', '##u', '##w', '##w;', '<UNK>', 'a', 'arrow;', 'b', 'bo', 'f', 'fl', 'fru', 'h', 'k', 'kn', 'l', 'm', 'o', 'ol', 'old', 's', 'sh', 't']\n",
        "    inputs = [['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>'], ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']]\n",
        "    correct_ans = ['she <UNK> the book <UNK>', 'time <UNK> for <UNK> one.']\n",
        "\n",
        "    overall_pass = True\n",
        "    for i in range(len(inputs)):\n",
        "        sent = tokenizer.detokenize(inputs[i])\n",
        "        if sent == correct_ans[i]: pass1, msg1 = 'CORRECT', ''\n",
        "        else:\n",
        "            pass1, msg1 = 'INCORRECT', 'Sentence is incorrect.'\n",
        "            overall_pass = False\n",
        "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\ttokens: ' + str(inputs[i]) +\n",
        "              '\\tYour sentence: ' + str(sent) + '\\tCorrect sentence: ' + str(correct_ans[i]))\n",
        "\n",
        "    if overall_pass: print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "\n",
        "def sanityCheckTokenizer():\n",
        "    sample_train_corpus = [\"the old man the boat.\",\n",
        "                   \"time flies like an arrow; fruit flies like a banana.\",\n",
        "                   \"she told him that she knew that he lied.\"]\n",
        "    sample_test_corpus = [\"she read the book 1984.\",\n",
        "                          \"time waits for no one.\"]\n",
        "\n",
        "    print(\"Sample train corpus:\", sample_train_corpus)\n",
        "    print(\"Sample test corpus:\", sample_test_corpus)\n",
        "    sanityCheckInitialize(sample_train_corpus)\n",
        "    sanityCheckFindBestPair(sample_train_corpus)\n",
        "    sanityCheckMergeBestPair(sample_train_corpus)\n",
        "    sanityCheckTokenize(sample_train_corpus, sample_test_corpus)\n",
        "    sanityCheckDetokenize(sample_train_corpus)\n"
      ],
      "metadata": {
        "id": "1h6jCy1WL9WG",
        "cellView": "form"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckTokenizer()"
      ],
      "metadata": {
        "id": "cQvj7i7S9_Ru",
        "outputId": "ce9d0e75-e237-42a5-e72e-3392e349c4de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample train corpus: ['the old man the boat.', 'time flies like an arrow; fruit flies like a banana.', 'she told him that she knew that he lied.']\n",
            "Sample test corpus: ['she read the book 1984.', 'time waits for no one.']\n",
            "\n",
            "\n",
            "--- TEST: initialize(self, train_corpus) ---\n",
            "\tvocab:\t\t\tCORRECT\t\tYour vocab: {'##n', 'a', '##k', '##u', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##r', 'm', '<UNK>', '##.', '##;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab:{'##n', 'a', '##k', '##u', 'm', '<UNK>', 's', '##;', '##.', 't', '##h', '##i', '##l', 'f', '##d', '##s', 'l', '##w', 'b', 'k', '##t', 'h', '##a', '##e', ' ', '##o', '##m', 'o', '##r'}\n",
            "\tword_tokenizations:\tCORRECT\t\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\n",
            "\tword_freqs:\t\tCORRECT\t\tYour word_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tCorrect word_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\n",
            "\ttokens2word:\t\tCORRECT\t\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'man', 'that'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}\n",
            "\n",
            "  Passed!\n",
            "\n",
            "\n",
            "--- TEST: find_best_pair(self, word_tokenizations, word_freqs, vocab) ---\n",
            "\tCase  0:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', 'm', '<UNK>', 's', '##;', '##.', 't', '##h', '##i', '##l', 'f', '##d', '##s', 'l', '##w', 'b', 'k', '##t', 'h', '##a', '##e', ' ', '##o', '##m', 'o', '##r'}\tYour best_pair: ('##w', '##;')\tCorrect best_pair: ('##w', '##;')\tYour scores: {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223}\tCorrect scores: {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223}\n",
            "\tCase  1:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', 'm', '##w;', 's', '<UNK>', '##;', '##.', 't', '##h', '##i', '##l', 'f', '##d', '##s', 'l', '##w', 'b', 'k', '##t', 'h', '##a', '##e', ' ', '##o', '##m', 'o', '##r'}\tYour best_pair: ('##o', '##w;')\tCorrect best_pair: ('##o', '##w;')\tYour scores: {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335}\tCorrect scores: {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335}\n",
            "\tCase  2:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', 'm', '##w;', 's', '<UNK>', '##;', '##.', 't', '##h', '##i', '##l', 'f', '##d', '##s', 'l', '##w', 'b', 'k', '##t', 'h', '##a', '##e', ' ', '##ow;', '##o', '##m', 'o', '##r'}\tYour best_pair: ('##r', '##ow;')\tCorrect best_pair: ('##r', '##ow;')\tYour scores: {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446}\tCorrect scores: {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446}\n",
            "\tCase  3:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('##r', '##row;')\tCorrect best_pair: ('##r', '##row;')\tYour scores: {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554}\tCorrect scores: {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554}\n",
            "\tCase  4:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('##r', '##u')\tCorrect best_pair: ('##r', '##u')\tYour scores: {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665}\tCorrect scores: {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665}\n",
            "\tCase  5:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('a', '##rrow;')\tCorrect best_pair: ('a', '##rrow;')\tYour scores: {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777}\tCorrect scores: {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777}\n",
            "\tCase  6:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('f', '##ru')\tCorrect best_pair: ('f', '##ru')\tYour scores: {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889}\tCorrect scores: {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889}\n",
            "\tCase  7:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('b', '##o')\tCorrect best_pair: ('b', '##o')\tYour scores: {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0}\tCorrect scores: {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0}\n",
            "\tCase  8:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'bo', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('##o', '##l')\tCorrect best_pair: ('##o', '##l')\tYour scores: {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111}\tCorrect scores: {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111}\n",
            "\tCase  9:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ol', '##ow;', 'bo', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('##ol', '##d')\tCorrect best_pair: ('##ol', '##d')\tYour scores: {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222}\tCorrect scores: {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222}\n",
            "\tCase 10:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('f', '##l')\tCorrect best_pair: ('f', '##l')\tYour scores: {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5}\tCorrect scores: {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5}\n",
            "\tCase 11:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'fl', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('o', '##l')\tCorrect best_pair: ('o', '##l')\tYour scores: {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667}\tCorrect scores: {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667}\n",
            "\tCase 12:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'fl', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('ol', '##d')\tCorrect best_pair: ('ol', '##d')\tYour scores: {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333}\tCorrect scores: {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333}\n",
            "\tCase 13:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('##d', '##.')\tCorrect best_pair: ('##d', '##.')\tYour scores: {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0}\tCorrect scores: {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0}\n",
            "\tCase 14:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('k', '##n')\tCorrect best_pair: ('k', '##n')\tYour scores: {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335}\tCorrect scores: {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335}\n",
            "\tCase 15:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', 'kn', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour best_pair: ('s', '##h')\tCorrect best_pair: ('s', '##h')\tYour scores: {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}\tCorrect scores: {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}\n",
            "\n",
            "  Passed!\n",
            "\n",
            "\n",
            "--- TEST: merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word) ---\n",
            "\tCase  0:\tCORRECT\t\tbest_pair: ('##w', '##;')\tvocab: {'##n', 'a', '##k', '##u', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##r', 'm', '<UNK>', '##;', '##.', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##r', 'm', '<UNK>', '##;', '##.', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', 'm', '##w;', 's', '<UNK>', '##;', '##.', 't', '##h', '##i', '##l', 'f', '##d', '##s', 'l', '##w', 'b', 'k', '##t', 'h', '##a', '##e', ' ', '##o', '##m', 'o', '##r'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}}\n",
            "\tCase  1:\tCORRECT\t\tbest_pair: ('##o', '##w;')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##r', 'm', '<UNK>', '##;', '##.', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', 'm', '##w;', 's', '<UNK>', '##;', '##.', 't', '##h', '##i', '##l', 'f', '##d', '##s', 'l', '##w', 'b', 'k', '##t', 'h', '##a', '##e', ' ', '##ow;', '##o', '##m', 'o', '##r'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}\n",
            "\tCase  2:\tCORRECT\t\tbest_pair: ('##r', '##ow;')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}\n",
            "\tCase  3:\tCORRECT\t\tbest_pair: ('##r', '##row;')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}\n",
            "\tCase  4:\tCORRECT\t\tbest_pair: ('##r', '##u')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}\n",
            "\tCase  5:\tCORRECT\t\tbest_pair: ('a', '##rrow;')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}\n",
            "\tCase  6:\tCORRECT\t\tbest_pair: ('f', '##ru')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}\n",
            "\tCase  7:\tCORRECT\t\tbest_pair: ('b', '##o')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'arrow;', 'bo', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'bo', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}\n",
            "\tCase  8:\tCORRECT\t\tbest_pair: ('##o', '##l')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ow;', 'bo', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ol', '##ow;', 'bo', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ol', '##ow;', 'bo', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}\n",
            "\tCase  9:\tCORRECT\t\tbest_pair: ('##ol', '##d')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ol', '##ow;', 'bo', 'arrow;', '##r', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}\n",
            "\tCase 10:\tCORRECT\t\tbest_pair: ('f', '##l')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'fl', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'fl', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}\n",
            "\tCase 11:\tCORRECT\t\tbest_pair: ('o', '##l')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'fl', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'fl', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'fl', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}\n",
            "\tCase 12:\tCORRECT\t\tbest_pair: ('ol', '##d')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'fl', '##ol', '##ow;', 'bo', 'arrow;', '##r', '##old', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'fl', '##ol', '##ow;', 'bo', 'arrow;', 'old', '##r', '##old', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}\n",
            "\tCase 13:\tCORRECT\t\tbest_pair: ('##d', '##.')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}\n",
            "\tCase 14:\tCORRECT\t\tbest_pair: ('k', '##n')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', 'kn', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', 'kn', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}\n",
            "\tCase 15:\tCORRECT\t\tbest_pair: ('s', '##h')\tvocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', 'kn', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', '##o', '##m', 'o'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\ttokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'fruit', 'lied.', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}\tYour vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', 'kn', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', 'sh', '##o', '##m', 'o'}\tCorrect vocab: {'##n', 'a', '##k', '##u', '##w;', 's', 'fru', '##d.', '##i', '##rrow;', '##l', '##d', 'l', '##w', 'b', '##t', '##a', 'old', '##ol', '##ow;', 'bo', 'fl', 'arrow;', '##old', '##r', 'm', 'kn', '<UNK>', '##;', '##.', 'ol', '##ru', '##row;', 't', '##h', 'f', '##s', 'k', 'h', '##e', ' ', 'sh', '##o', '##m', 'o'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tYour tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'told', 'old', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'like', 'lied.', 'fruit', 'flies', 'him', 'time'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}\tCorrect tokens2word: {'t': {'told', 'time', 'that', 'the'}, '##h': {'she', 'that', 'the'}, '##e': {'knew', 'he', 'she', 'like', 'lied.', 'flies', 'time', 'the'}, 'o': {'old'}, '##l': {'flies', 'told', 'old'}, '##d': {'lied.', 'told', 'old'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'knew', 'an', 'banana.', 'man'}, 'b': {'boat.', 'banana.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'boat.', 'banana.', 'lied.'}, '##i': {'flies', 'him', 'lied.', 'like', 'time', 'fruit'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}\n",
            "\n",
            "  Passed!\n",
            "\n",
            "\n",
            "--- TEST: tokenize(self, sentence) ---\n",
            "\tCase  0:\tCORRECT\t\tsentence: she read the book 1984.\tYour tokenization: ['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>']\tCorrect tokenization: ['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>']\n",
            "\tCase  1:\tCORRECT\t\tsentence: time waits for no one.\tYour tokenization: ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']\tCorrect tokenization: ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']\n",
            "\n",
            "  Passed!\n",
            "\n",
            "\n",
            "--- TEST: detokenize(self, tokens) ---\n",
            "\tCase  0:\tCORRECT\t\ttokens: ['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>']\tYour sentence: she <UNK> the book <UNK>\tCorrect sentence: she <UNK> the book <UNK>\n",
            "\tCase  1:\tCORRECT\t\ttokens: ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']\tYour sentence: time <UNK> for <UNK> one.\tCorrect sentence: time <UNK> for <UNK> one.\n",
            "\n",
            "  Passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run your tokenizer\n",
        "\n",
        "Now that you have written your tokenizer, we can run it on real data. If implemented correctly. your tokenizer should train in **less than 4 minutes**. If your code takes too long, make sure your `merge_best_pair(best_pair, ...)` function uses the `tokens2word` dictionary to only iterate over the words that use one of the tokens in `best_pair`."
      ],
      "metadata": {
        "id": "bCbUEgXfLz81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### DO NOT EDIT ###\n",
        "\n",
        "if __name__=='__main__':\n",
        "    tokenizer = WordPieceTokenizer(dataset.train, 700, do_tqdm = True, do_print=False)\n",
        "    tokenizer.train()\n",
        "    print(\"Vocab:\", tokenizer.vocab)"
      ],
      "metadata": {
        "id": "lPdN2aTVvTu9",
        "outputId": "438f4905-62f8-44a6-ef9f-b157e0e2190d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "1fce0425160542619b62126e383b15f3",
            "57f929e9cc084e65ae69841b1bab331b",
            "524b8d84ea1643dca3bf03d55b4c2326",
            "d3b2a41052bd4363a738f6e5e961c688",
            "6037111619b14f9ebcfc25beaf684e02",
            "d91fb02701954eac901f1301fd133eff",
            "176007c44d3e457fa79a7b3eabfce0d0",
            "0201e6f0ea5f4b1580e0372788170eb3",
            "8a6e39890f4648c48f7bdddd77236e6b",
            "2224c23499934fa7854d90f261bfeb8c",
            "d881403734d744e09322079bceb0a6f9"
          ]
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/603 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fce0425160542619b62126e383b15f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab: [' ', '#', '##&', '##(', '##(m', '##(p', '##(po', '##(por', '##(port', '##(porth', '##(porthm', '##(porthma', '##(porthmad', '##(porthmado', '##(porthmadog', '##(porthmadog,', '##)', '##),', '##).', '##*', '##,', '##-', '##-b', '##-f', '##-guzzl', '##-j', '##-ju', '##-jud', '##-judg', '##-jum', '##-jump', '##-jur', '##-jury', '##-q', '##-qu', '##-qua', '##-quak', '##-qual', '##-quali', '##-qualit', '##-quality', '##-quar', '##-quart', '##.', '##/', '##/f', '##/h', '##:', '##;', '##>', '##>>', '##]', '##].', '##^', '##^^', '##a', '##ack-fr', '##ack].', '##addam┐s', '##aff)', '##afé', '##aid:', '##am┐s', '##aq', '##aquí', '##as-guzzl', '##aturday].', '##ax-fr', '##ay].', '##azz', '##azzi,', '##azzl', '##b', '##b&', '##back].', '##bj', '##bjo', '##bjoh', '##c', '##ck', '##ck-f', '##ck-fr', '##ck].', '##d', '##d:', '##dam┐s', '##day].', '##ddam┐s', '##dn’t', '##dq', '##d|', '##e', '##f', '##f),', '##faq', '##ff', '##ff)', '##ff-f', '##fuq', '##fé', '##g', '##g-jump', '##g-jumpi', '##g-jumpin', '##g-jumping', '##g].', '##gq', '##gqi', '##gqin', '##gqing', '##gqv', '##guzzl', '##h', '##hfaq', '##hongqing', '##i', '##id:', '##idn’t', '##ing].', '##ion┐.', '##ission┐.', '##ix).', '##ix-h', '##ix-m', '##ix-w', '##ix-y', '##izzim', '##izzy', '##iņ', '##j', '##jpq', '##k', '##k/', '##k/u', '##k].', '##l', '##l-q', '##lack-fr', '##loqx', '##lts/f', '##lying].', '##m', '##mq', '##m┐', '##m┐s', '##n', '##nazzi,', '##nd|', '##ng-jumping', '##ng].', '##ngqing', '##ntys(mo', '##né', '##n’t', '##n┐.', '##o', '##o-jury', '##oaquí', '##objoh', '##ong-jumping', '##ongqing', '##ontys(mo', '##on’t', '##on┐.', '##op|', '##oqx', '##ost-quak', '##p', '##pf),', '##plying].', '##pplying].', '##pq', '##p|', '##q', '##q.', '##qa', '##qb', '##qu', '##quí', '##qx', '##r', '##raq', '##razzl', '##rday].', '##rgqv', '##rgqvi', '##rgqvis', '##rgqvist', '##s', '##s(m', '##s-guzzl', '##s/f', '##shfaq', '##sion┐.', '##ssion┐.', '##st-quak', '##sults/f', '##sults/fi', '##sults/fix', '##sults/fixt', '##sults/fixtu', '##sults/fixtur', '##t', '##t-quak', '##ts/f', '##tuff-f', '##turday].', '##tys(m', '##tys(mo', '##u', '##ubj', '##udq', '##uff-f', '##uk/u', '##ults/f', '##upplying].', '##uq', '##urday].', '##uzz', '##uzzl', '##uí', '##v', '##w', '##x', '##x).', '##x-', '##x-f', '##x-fr', '##x-h', '##x-m', '##x-w', '##x-y', '##y', '##y].', '##ying].', '##ys(m', '##z', '##zz', '##zzi', '##zzi,', '##zzim', '##zzl', '##zzy', '##|', '##|.', '##©', '##¿', '##¿h', '##¿ha', '##¿hav', '##ä', '##é', '##í', '##ó', '##ņ', '##’', '##’d', '##’s', '##’t', '##”', '##…', '##┐', '##┐.', '##┐s', '#b', '#br', '#bri', '#brin', '#bring', '#bringb', '#bringba', '#bringbac', '#bringback', '#bringbacko', '#bringbackou', '#bringbackour', '#bringbackourg', '#bringbackourgi', '#bringbackourgir', '#bringbackourgirl', '#bringbackourgirls', '#p', '#pi', '#pir', '#pira', '#pirat', '&', '(', '(aff)', '(bj', '(bjp', '(bjp)', '(c', '(c)', '(ch', '(chp', '(chp)', '(cp', '(cps', '(cps)', '(cs', '(css', '(css)', '(f', '(fg', '(fgm', '(fgm).', '(fgw', '(fgw)', '(j', '(jv', '(jvs', '(jvs)', '(mq', '(mqm', '(mqm),', '(pf),', '(uk/u', '(uk/us', '(uk/usa', '(uk/usa)', '(w', '(wh', ')', ',', '-', ':', '<UNK>', '>', '>>', '@', '@a', '@an', '@and', '@andr', '@d', '[', '[back].', '[f', '[w', '`', '`l', '`lo', '`lon', '`long', 'a', 'a&', 'a*', 'a*s', 'al-q', 'al-qa', 'aq', 'ashfaq', 'audq', 'b', 'bb&', 'bb&t', 'black-fr', 'black-fra', 'black-fram', 'buzz', 'bä', 'bäc', 'bäck', 'bäckh', 'c', 'café', 'chongqing', 'd', 'd&', 'd&d', 'dazzl', 'didn’t', 'don’t', 'dí', 'dít', 'dítr', 'e', 'e-b', 'e-bo', 'e-boo', 'e-book', 'e-bor', 'e-bord', 'eff', 'equ', 'ev', 'ex', 'ex-', 'ex-b', 'ex-br', 'ex-bri', 'ex-brit', 'ex-briti', 'ex-britis', 'ex-british', 'ex-c', 'ex-ch', 'ex-cha', 'ex-chan', 'ex-chanc', 'ex-ci', 'ex-cia', 'ex-g', 'ex-gu', 'ex-o', 'ex-off', 'ex-p', 'ex-s', 'ex-so', 'ex-sov', 'ex-sovi', 'ex-v', 'ex-vi', 'ex-vic', 'exc', 'exp', 'f', 'fizzy', 'frazzl', 'fé', 'féi', 'féin', 'g', 'gas-guzzl', 'gazz', 'h', 'i', 'iq.', 'iqb', 'iqba', 'iqbal', 'iraq', 'iraq.', 'ishfaq', 'i’', 'i’m', 'j', 'ja', 'jack', 'jack,', 'jam', 'jap', 'japa', 'japan', 'jj', 'jjm', 'jjmm', 'jo', 'joaquí', 'joaquín', 'joaquín,', 'job', 'job.', 'jock', 'joff', 'joh', 'joha', 'john', 'jok', 'jou', 'joua', 'jouan', 'jour', 'journ', 'journa', 'journal', 'journal,', 'journal.', 'journal:', 'journali', 'journalis', 'journalism', 'journalism.', 'journalist', 'journalist:', 'ju', 'jub', 'juba', 'jubi', 'jubil', 'jud', 'judg', 'judi', 'judic', 'judici', 'judicia', 'judicial', 'judiciar', 'judiciary', 'jus', 'juss', 'jussi', 'just', 'jux', 'juxt', 'juxta', 'juxtap', 'juxtapo', 'juxtapos', 'juxtaposi', 'juxtaposit', 'juxtapositi', 'juxtapositio', 'juxtaposition', 'jz', 'jzb', 'jzbu', 'k', 'l', 'long-jumping', 'm', 'mission┐.', 'mix).', 'mizzim', 'mizzima', 'mjpq', 'montys(mo', 'montys(mon', 'montys(mont', 'montys(montg', 'montys(montgo', 'montys(montgom', 'mó', 'mór', 'n', 'niņ', 'niņa', 'no-jury', 'o', 'obj', 'of', 'off', 'off-', 'off-p', 'off-pu', 'off-put', 'off-putt', 'off-putti', 'off-puttin', 'off-putting', 'off-putting,', 'p', 'pfuq', 'post-quak', 'puzzl', 'q', 'q:', 'qu', 'r', 'r&', 'r&b', 'razzl', 'robjoh', 'robjohn', 'robjohn,', 's', 'saddam┐s', 'said:', 'saturday].', 'six-h', 'six-ho', 'six-hou', 'six-hour', 'six-m', 'six-mo', 'six-mon', 'six-mont', 'six-month', 'six-w', 'six-y', 'stuff-f', 'stuff-fi', 'stuff-fil', 'stuff-fill', 'subj', 'supplying].', 't', 'tax-fr', 'th', 'tizzy', 'to', 'top|', 'tw', 'twi', 'twic', 'twick', 'twig', 'twigg', 'twigg,', 'twin', 'twin-', 'twis', 'twist', 'twit', 'twitc', 'twitch', 'twitt', 'two', 'two-', 'two-m', 'two-p', 'two-pr', 'two-pro', 'two-pron', 'two-prong', 'two-w', 'two-y', 'u', 'uk', 'uk)', 'uk,', 'uk-b', 'uk-ba', 'uk-bas', 'uk.', 'uk’', 'uk’s', 'up', 'up-', 'up-c', 'up-co', 'up-com', 'up-comi', 'up-comin', 'up-coming', 'up-f', 'up-fr', 'up-fro', 'up-fron', 'up-front', 'up-t', 'up-to', 'up-to-', 'up-to-d', 'up-to-da', 'up-to-dat', 'uq', 'uqr', 'uqrd', 'v', 'w', 'waq', 'waqt', 'wh', 'x', 'x-', 'x-r', 'x-ra', 'x-rat', 'x-ray', 'xz', 'xzc', 'xzca', 'y', 'yo', 'yob', 'yobb', 'yobbi', 'yobbis', 'yobbish', 'yobo', 'yok', 'yoko', 'yokoh', 'yokoha', 'yokoham', 'yokohama', 'yokohama,', 'yoo', 'yor', 'york', 'york,', 'york.', 'you', 'yow', 'z', '|', '|^^', '|©', '|©m', '|©mm', '|©mmv', '|©mmvi', '|©mmvii', '|©mmviii', '©', 'ó', '“', '“w', '“wa', '“wan', '“want', '”', '•', '┐', '┐a', '┐ac', '┐ach', '┐achi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can look at some tokenizations on the test set."
      ],
      "metadata": {
        "id": "sCR5Iq6oo6wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### DO NOT EDIT ###\n",
        "\n",
        "if __name__=='__main__':\n",
        "    for sent in random.sample(dataset.test, 10):\n",
        "        print('Sentence:', sent)\n",
        "        toks = tokenizer.tokenize(sent)\n",
        "        print('Tokenization:', toks)\n",
        "        print('Detokenization:', tokenizer.detokenize(toks))\n",
        "        print()"
      ],
      "metadata": {
        "id": "csdUptctgyVu",
        "outputId": "e17a916a-6fb8-40df-dc39-92d670afe8c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: live aid was pretty shambolic, he said.\n",
            "Tokenization: ['l', '##i', '##v', '##e', ' ', 'a', '##i', '##d', ' ', 'w', '##a', '##s', ' ', 'p', '##r', '##e', '##t', '##t', '##y', ' ', 's', '##h', '##a', '##m', '##b', '##o', '##l', '##i', '##c', '##,', ' ', 'h', '##e', ' ', 's', '##a', '##i', '##d', '##.']\n",
            "Detokenization: live aid was pretty shambolic, he said.\n",
            "\n",
            "Sentence: without water they cannot irrigate, so they cannot feed themselves.\n",
            "Tokenization: ['w', '##i', '##t', '##h', '##o', '##u', '##t', ' ', 'w', '##a', '##t', '##e', '##r', ' ', 'th', '##e', '##y', ' ', 'c', '##a', '##n', '##n', '##o', '##t', ' ', 'i', '##r', '##r', '##i', '##g', '##a', '##t', '##e', '##,', ' ', 's', '##o', ' ', 'th', '##e', '##y', ' ', 'c', '##a', '##n', '##n', '##o', '##t', ' ', 'f', '##e', '##e', '##d', ' ', 'th', '##e', '##m', '##s', '##e', '##l', '##v', '##e', '##s', '##.']\n",
            "Detokenization: without water they cannot irrigate, so they cannot feed themselves.\n",
            "\n",
            "Sentence: many people might have been looking at what we were going to do today and see if we were going to fall flat on our face - i thought we gave a great response.\n",
            "Tokenization: ['m', '##a', '##n', '##y', ' ', 'p', '##e', '##o', '##p', '##l', '##e', ' ', 'm', '##i', '##g', '##h', '##t', ' ', 'h', '##a', '##v', '##e', ' ', 'b', '##e', '##e', '##n', ' ', 'l', '##o', '##o', '##k', '##i', '##n', '##g', ' ', 'a', '##t', ' ', 'wh', '##a', '##t', ' ', 'w', '##e', ' ', 'w', '##e', '##r', '##e', ' ', 'g', '##o', '##i', '##n', '##g', ' ', 'to', ' ', 'd', '##o', ' ', 'to', '##d', '##a', '##y', ' ', 'a', '##n', '##d', ' ', 's', '##e', '##e', ' ', 'i', '##f', ' ', 'w', '##e', ' ', 'w', '##e', '##r', '##e', ' ', 'g', '##o', '##i', '##n', '##g', ' ', 'to', ' ', 'f', '##a', '##l', '##l', ' ', 'f', '##l', '##a', '##t', ' ', 'o', '##n', ' ', 'o', '##u', '##r', ' ', 'f', '##a', '##c', '##e', ' ', '-', ' ', 'i', ' ', 'th', '##o', '##u', '##g', '##h', '##t', ' ', 'w', '##e', ' ', 'g', '##a', '##v', '##e', ' ', 'a', ' ', 'g', '##r', '##e', '##a', '##t', ' ', 'r', '##e', '##s', '##p', '##o', '##n', '##s', '##e', '##.']\n",
            "Detokenization: many people might have been looking at what we were going to do today and see if we were going to fall flat on our face - i thought we gave a great response.\n",
            "\n",
            "Sentence: adobe has signed up video delivery service netflix, disney and the new york times to make the first batch of applications.\n",
            "Tokenization: ['a', '##d', '##o', '##b', '##e', ' ', 'h', '##a', '##s', ' ', 's', '##i', '##g', '##n', '##e', '##d', ' ', 'up', ' ', 'v', '##i', '##d', '##e', '##o', ' ', 'd', '##e', '##l', '##i', '##v', '##e', '##r', '##y', ' ', 's', '##e', '##r', '##v', '##i', '##c', '##e', ' ', 'n', '##e', '##t', '##f', '##l', '##i', '##x', '##,', ' ', 'd', '##i', '##s', '##n', '##e', '##y', ' ', 'a', '##n', '##d', ' ', 'th', '##e', ' ', 'n', '##e', '##w', ' ', 'york', ' ', 't', '##i', '##m', '##e', '##s', ' ', 'to', ' ', 'm', '##a', '##k', '##e', ' ', 'th', '##e', ' ', 'f', '##i', '##r', '##s', '##t', ' ', 'b', '##a', '##t', '##c', '##h', ' ', 'of', ' ', 'a', '##p', '##p', '##l', '##i', '##c', '##a', '##t', '##i', '##o', '##n', '##s', '##.']\n",
            "Detokenization: adobe has signed up video delivery service netflix, disney and the new york times to make the first batch of applications.\n",
            "\n",
            "Sentence: lead research professor allen wilcox said: there apparently are biological factors promoting intercourse during a womans six fertile days, whether she wants a baby or not.\n",
            "Tokenization: ['l', '##e', '##a', '##d', ' ', 'r', '##e', '##s', '##e', '##a', '##r', '##c', '##h', ' ', 'p', '##r', '##o', '##f', '##e', '##s', '##s', '##o', '##r', ' ', 'a', '##l', '##l', '##e', '##n', ' ', 'w', '##i', '##l', '##c', '##o', '##x', ' ', 'said:', ' ', 'th', '##e', '##r', '##e', ' ', 'a', '##p', '##p', '##a', '##r', '##e', '##n', '##t', '##l', '##y', ' ', 'a', '##r', '##e', ' ', 'b', '##i', '##o', '##l', '##o', '##g', '##i', '##c', '##a', '##l', ' ', 'f', '##a', '##c', '##t', '##o', '##r', '##s', ' ', 'p', '##r', '##o', '##m', '##o', '##t', '##i', '##n', '##g', ' ', 'i', '##n', '##t', '##e', '##r', '##c', '##o', '##u', '##r', '##s', '##e', ' ', 'd', '##u', '##r', '##i', '##n', '##g', ' ', 'a', ' ', 'w', '##o', '##m', '##a', '##n', '##s', ' ', 's', '##i', '##x', ' ', 'f', '##e', '##r', '##t', '##i', '##l', '##e', ' ', 'd', '##a', '##y', '##s', '##,', ' ', 'wh', '##e', '##t', '##h', '##e', '##r', ' ', 's', '##h', '##e', ' ', 'w', '##a', '##n', '##t', '##s', ' ', 'a', ' ', 'b', '##a', '##b', '##y', ' ', 'o', '##r', ' ', 'n', '##o', '##t', '##.']\n",
            "Detokenization: lead research professor allen wilcox said: there apparently are biological factors promoting intercourse during a womans six fertile days, whether she wants a baby or not.\n",
            "\n",
            "Sentence: it is depressing that peoples lives are so sad that they feel they have to watch it.\n",
            "Tokenization: ['i', '##t', ' ', 'i', '##s', ' ', 'd', '##e', '##p', '##r', '##e', '##s', '##s', '##i', '##n', '##g', ' ', 'th', '##a', '##t', ' ', 'p', '##e', '##o', '##p', '##l', '##e', '##s', ' ', 'l', '##i', '##v', '##e', '##s', ' ', 'a', '##r', '##e', ' ', 's', '##o', ' ', 's', '##a', '##d', ' ', 'th', '##a', '##t', ' ', 'th', '##e', '##y', ' ', 'f', '##e', '##e', '##l', ' ', 'th', '##e', '##y', ' ', 'h', '##a', '##v', '##e', ' ', 'to', ' ', 'w', '##a', '##t', '##c', '##h', ' ', 'i', '##t', '##.']\n",
            "Detokenization: it is depressing that peoples lives are so sad that they feel they have to watch it.\n",
            "\n",
            "Sentence: indeed, ive only just married my british-born pakistani girlfriend (im white), and have found her family and culture to be very open and welcoming.\n",
            "Tokenization: ['i', '##n', '##d', '##e', '##e', '##d', '##,', ' ', 'i', '##v', '##e', ' ', 'o', '##n', '##l', '##y', ' ', 'just', ' ', 'm', '##a', '##r', '##r', '##i', '##e', '##d', ' ', 'm', '##y', ' ', 'b', '##r', '##i', '##t', '##i', '##s', '##h', '##-b', '##o', '##r', '##n', ' ', 'p', '##a', '##k', '##i', '##s', '##t', '##a', '##n', '##i', ' ', 'g', '##i', '##r', '##l', '##f', '##r', '##i', '##e', '##n', '##d', ' ', '(', '##i', '##m', ' ', 'wh', '##i', '##t', '##e', '##),', ' ', 'a', '##n', '##d', ' ', 'h', '##a', '##v', '##e', ' ', 'f', '##o', '##u', '##n', '##d', ' ', 'h', '##e', '##r', ' ', 'f', '##a', '##m', '##i', '##l', '##y', ' ', 'a', '##n', '##d', ' ', 'c', '##u', '##l', '##t', '##u', '##r', '##e', ' ', 'to', ' ', 'b', '##e', ' ', 'v', '##e', '##r', '##y', ' ', 'o', '##p', '##e', '##n', ' ', 'a', '##n', '##d', ' ', 'w', '##e', '##l', '##c', '##o', '##m', '##i', '##n', '##g', '##.']\n",
            "Detokenization: indeed, ive only just married my british-born pakistani girlfriend (im white), and have found her family and culture to be very open and welcoming.\n",
            "\n",
            "Sentence: this aims to turn the power of our own immune systems against tumours.\n",
            "Tokenization: ['th', '##i', '##s', ' ', 'a', '##i', '##m', '##s', ' ', 'to', ' ', 't', '##u', '##r', '##n', ' ', 'th', '##e', ' ', 'p', '##o', '##w', '##e', '##r', ' ', 'of', ' ', 'o', '##u', '##r', ' ', 'o', '##w', '##n', ' ', 'i', '##m', '##m', '##u', '##n', '##e', ' ', 's', '##y', '##s', '##t', '##e', '##m', '##s', ' ', 'a', '##g', '##a', '##i', '##n', '##s', '##t', ' ', 't', '##u', '##m', '##o', '##u', '##r', '##s', '##.']\n",
            "Detokenization: this aims to turn the power of our own immune systems against tumours.\n",
            "\n",
            "Sentence: local plans are at the heart of this and give councils more power to shape were development should go.\n",
            "Tokenization: ['l', '##o', '##c', '##a', '##l', ' ', 'p', '##l', '##a', '##n', '##s', ' ', 'a', '##r', '##e', ' ', 'a', '##t', ' ', 'th', '##e', ' ', 'h', '##e', '##a', '##r', '##t', ' ', 'of', ' ', 'th', '##i', '##s', ' ', 'a', '##n', '##d', ' ', 'g', '##i', '##v', '##e', ' ', 'c', '##o', '##u', '##n', '##c', '##i', '##l', '##s', ' ', 'm', '##o', '##r', '##e', ' ', 'p', '##o', '##w', '##e', '##r', ' ', 'to', ' ', 's', '##h', '##a', '##p', '##e', ' ', 'w', '##e', '##r', '##e', ' ', 'd', '##e', '##v', '##e', '##l', '##o', '##p', '##m', '##e', '##n', '##t', ' ', 's', '##h', '##o', '##u', '##l', '##d', ' ', 'g', '##o', '##.']\n",
            "Detokenization: local plans are at the heart of this and give councils more power to shape were development should go.\n",
            "\n",
            "Sentence: mr qarase told the bbcs world today programme on thursday that he would introduce a revised bill ordering parliament to set up a commission to investigate the coup.\n",
            "Tokenization: ['m', '##r', ' ', 'q', '##a', '##r', '##a', '##s', '##e', ' ', 'to', '##l', '##d', ' ', 'th', '##e', ' ', 'b', '##b', '##c', '##s', ' ', 'w', '##o', '##r', '##l', '##d', ' ', 'to', '##d', '##a', '##y', ' ', 'p', '##r', '##o', '##g', '##r', '##a', '##m', '##m', '##e', ' ', 'o', '##n', ' ', 'th', '##u', '##r', '##s', '##d', '##a', '##y', ' ', 'th', '##a', '##t', ' ', 'h', '##e', ' ', 'w', '##o', '##u', '##l', '##d', ' ', 'i', '##n', '##t', '##r', '##o', '##d', '##u', '##c', '##e', ' ', 'a', ' ', 'r', '##e', '##v', '##i', '##s', '##e', '##d', ' ', 'b', '##i', '##l', '##l', ' ', 'o', '##r', '##d', '##e', '##r', '##i', '##n', '##g', ' ', 'p', '##a', '##r', '##l', '##i', '##a', '##m', '##e', '##n', '##t', ' ', 'to', ' ', 's', '##e', '##t', ' ', 'up', ' ', 'a', ' ', 'c', '##o', '##m', '##m', '##i', '##s', '##s', '##i', '##o', '##n', ' ', 'to', ' ', 'i', '##n', '##v', '##e', '##s', '##t', '##i', '##g', '##a', '##t', '##e', ' ', 'th', '##e', ' ', 'c', '##o', '##u', '##p', '##.']\n",
            "Detokenization: mr qarase told the bbcs world today programme on thursday that he would introduce a revised bill ordering parliament to set up a commission to investigate the coup.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKt-WVr6OtP4"
      },
      "source": [
        "# Part 2: Language Models [72 points]\n",
        "\n",
        "Here, you will train some <b>n-gram language models</b> on WikiText-2, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following paper: https://arxiv.org/pdf/1609.07843v1.pdf. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN2Ja8MNP4qS"
      },
      "source": [
        "## Download & preprocess the data\n",
        "\n",
        "To make your models more robust, it is necessary to perform some basic preprocessing on the corpora. <i>You do not need to edit this code.</i>\n",
        "\n",
        "* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp;In this homework, we are interested in modeling individual sentences, rather than longer chunks of text such as paragraphs or documents. The WikiTest dataset provides paragraphs; thus, we provide a simple method to identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n",
        "\n",
        "* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). These markers will allow your models to generate sentences that have realistic beginnings and endings.\n",
        "\n",
        "* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown words in the test corpora, all words that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`) before estimating your models. The WikiText dataset has already done this, and you can read about the method in the paper above. When unknown words are encountered in the test corpus, they should be treated as that special token instead.\n",
        "\n",
        "We provide you with preprocessing code here, and you should not modify it.\n",
        "\n",
        "After the preprocessing, you may assume that all words in the test set appear in the training set, as this code has already replaced the unseen tokens with `<UNK>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCkrUjKEBrNp"
      },
      "source": [
        "### DO NOT EDIT ###\n",
        "\n",
        "# Constants (feel free to use these in your code, but do not change them)\n",
        "START = \"<s>\"   # Start-of-sentence token\n",
        "END = \"</s>\"    # End-of-sentence-token\n",
        "UNK = \"<UNK>\"   # Unknown word token"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUdZstjH30DL",
        "outputId": "1fb9438a-35a1-408d-c47d-a9fec0a663b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### DO NOT EDIT ###\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "def preprocess(data, vocab=None):\n",
        "    final_data = []\n",
        "    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "    for paragraph in data:\n",
        "        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()]\n",
        "        if vocab is not None:\n",
        "            paragraph = [x if x in vocab else UNK for x in paragraph]\n",
        "        if paragraph == [] or paragraph.count('=') >= 2: continue\n",
        "        sen = []\n",
        "        prev_punct, prev_quot = False, False\n",
        "        for word in paragraph:\n",
        "            if prev_quot:\n",
        "                if word[0] not in lowercase:\n",
        "                    final_data.append(sen)\n",
        "                    sen = []\n",
        "                    prev_punct, prev_quot = False, False\n",
        "            if prev_punct:\n",
        "                if word == '\"':\n",
        "                    prev_punct, prev_quot = False, True\n",
        "                else:\n",
        "                    if word[0] not in lowercase:\n",
        "                        final_data.append(sen)\n",
        "                        sen = []\n",
        "                        prev_punct, prev_quot = False, False\n",
        "            if word in {'.', '?', '!'}: prev_punct = True\n",
        "            sen += [word]\n",
        "        if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n",
        "        final_data.append(sen)\n",
        "    vocab_was_none = vocab is None\n",
        "    if vocab is None:\n",
        "        vocab = set()\n",
        "    for i in range(len(final_data)):\n",
        "        final_data[i] = [START] + final_data[i] + [END]\n",
        "        if vocab_was_none:\n",
        "            for word in final_data[i]:\n",
        "                vocab.add(word)\n",
        "    return final_data, vocab\n",
        "\n",
        "def getDataset():\n",
        "    splits = ['train', 'valid']\n",
        "    datasets = []\n",
        "    path = './{}.txt'\n",
        "    url = 'https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/{}.txt'\n",
        "    for split in splits:\n",
        "        if os.path.exists(path.format(split)):\n",
        "            print(f\"{split} dataset already downloaded\")\n",
        "        else:\n",
        "            filename = f'{split}.txt'\n",
        "            urlretrieve(url.format(split), filename)\n",
        "        datasets.append(open(f'{split}.txt').read().split('\\n'))\n",
        "    train_dataset, vocab = preprocess(datasets[0])\n",
        "    test_dataset, _ = preprocess(datasets[1], vocab)\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_dataset, test_dataset = getDataset()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset already downloaded\n",
            "valid dataset already downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSFJ07ELGUMh"
      },
      "source": [
        "Run the next cell to see 10 random sentences of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swPwiHBHDDkT",
        "outputId": "45db0406-1200-4f4a-e05c-be27109a86e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    for x in random.sample(train_dataset, 10):\n",
        "        print (x)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'It', 'just', 'looks', 'right', '.', '</s>']\n",
            "['<s>', 'The', 'drive', 'eventually', 'resulted', 'in', 'a', 'game', '@-@', 'winning', 'touchdown', 'for', 'Cincinnati', '.', '</s>']\n",
            "['<s>', 'Poet', 'Sterling', 'D.', '<UNK>', 'found', 'Angelou', \"'s\", 'performance', '\"', 'brilliant', '\"', ',', 'but', 'was', '\"', 'not', 'as', 'enthusiastic', 'about', 'it', 'as', 'a', 'text', '\"', '.', '</s>']\n",
            "['<s>', 'After', 'Germany', \"'s\", 'defeat', 'in', 'the', 'war', 'and', 'the', 'signing', 'of', 'the', 'Armistice', 'in', 'November', '1918', ',', 'Markgraf', 'and', 'most', 'of', 'the', 'capital', 'ships', 'of', 'the', 'High', 'Seas', 'Fleet', 'were', 'interned', 'by', 'the', 'Royal', 'Navy', 'in', 'Scapa', 'Flow', '.', '</s>']\n",
            "['<s>', 'He', 'is', 'considered', 'one', 'of', 'the', 'most', 'influential', 'Kannada', 'poets', 'of', 'the', 'Hoysala', 'era', '.', '</s>']\n",
            "['<s>', 'Strong', 'winds', 'blowing', 'off', 'the', 'land', 'swept', 'the', 'smoke', 'screen', 'into', 'the', 'face', 'of', 'the', 'advancing', 'cruisers', ',', '<UNK>', 'their', 'commanders', 'who', 'attempted', 'to', 'navigate', 'by', 'dead', 'reckoning', '.', '</s>']\n",
            "['<s>', 'Around', '<UNK>', 'that', 'night', ',', 'another', 'witness', ',', 'a', 'woman', 'who', 'knew', 'Carol', ',', 'saw', 'her', 'at', '<UNK>', 'Richard', 'services', '.', '</s>']\n",
            "['<s>', 'The', 'World', 'Health', 'Organization', 'says', 'that', '<UNK>', 'lubricated', 'condoms', 'should', 'no', 'longer', 'be', 'promoted', '.', '</s>']\n",
            "['<s>', 'The', 'church', \"'s\", 'windows', 'have', 'been', 'repaired', 'using', 'materials', 'consistent', 'with', 'original', 'construction', '.', '</s>']\n",
            "['<s>', 'A.', 'maximus', 'was', 'coined', 'by', 'David', 'K.', 'Smith', 'for', 'Chure', \"'s\", 'Saurophaganax', 'maximus', ',', 'a', 'taxon', 'created', 'by', 'Chure', 'in', '1995', 'for', 'giant', '<UNK>', 'remains', 'from', 'the', 'Morrison', 'of', 'Oklahoma', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM6hNHMqTMt2"
      },
      "source": [
        "## The LanguageModel Class\n",
        "\n",
        "You will implement 4 types of language models: a <b>unigram</b> model, a <b>smoothed unigram</b> model, a <b>bigram</b> model, and a <b>smoothed bigram</b> model. Each of the models is worth 25 points and extends the following base class. <b>You do not need to implement anything in this class</b>; you will instead implement each of the following methods in the relevant subclass:\n",
        "\n",
        "* <b>`__init__(self, trainCorpus)`</b>: Train the language model on `trainCorpus`. This will involve calculating relative frequency estimates according to the type of model you're implementing.\n",
        "\n",
        "* <b>`generateSentence(self)`</b>: <b>[4 points]</b> Return a sentence that is generated by the language model. It should be a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>, where each <TT>w<sup>(i)</sup></TT> is a word in your vocabulary (including <TT>&lt;UNK&gt;</TT> but exlcuding <TT>&lt;s&gt;</TT> and <TT>&lt;&sol;s&gt;</TT>). You may assume that <TT>&lt;s&gt;</TT> starts each sentence (with probability $1$). The following words <TT>w<sup>(1)</sup></TT>, ... , <TT>w<sup>(n)</sup></TT>, <TT>&lt;&sol;s&gt;</TT> are generated according to your language model's distribution. Note that the number of words <TT>n</TT> is not fixed; instead, you should stop the sentence as soon as you generate the stop token <TT>&lt;&sol;s&gt;</TT>.\n",
        "\n",
        "* <b>`getSentenceLogProbability(self, sentence)`</b>: <b>[7 points]</b> Return the <em> logarithm of the probability</em> of <TT>sentence</TT>, which is again a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>. You should use the natural logarithm $-$ that is, the base-<em>e</em> logarithm. See the note below about performing your calculations in log space.\n",
        "\n",
        "* <b>`getCorpusPerplexity(self, testCorpus)`</b>: <b>[7 points]</b> You need to compute the perplexity (normalized inverse log probability) of `testCorpus` according to your model. For a corpus $W$ with $N$ words and a bigram model, Jurafsky and Martin tells you to compute perplexity as follows:\n",
        "\n",
        "$$Perplexity(W) = \\Big [ \\prod_{i=1}^N \\frac{1}{P(w^{(i)}|w^{(i-1)})} \\Big ]^{1/N}$$\n",
        "\n",
        "<b>Implementation Hint:</b> In order to avoid underflow, you will likely need to do all of your calculations in log-space. That is, instead of multiplying probabilities, you should add the logarithms of the probabilities and exponentiate the result:\n",
        "\n",
        "$$\\prod_{i=1}^N P(w^{(i)}|w^{(i-1)}) = \\exp\\Big (\\sum_{i=1}^N \\log P(w^{(i)}|w^{(i-1)}) \\Big ) $$\n",
        "\n",
        "Using this property should help you in your implementation of `generateSentence(self)` and `getCorpusPerplexity(self, testCorpus)`.\n",
        "\n",
        "Feel free to implement helper methods as you wish (either in the base class or in the subclases). <b>But be sure not to change the function signatures of the provided methods</b> (i.e. the function and argument names), or else the autograder will fail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKO6dHNSS45P",
        "cellView": "code"
      },
      "source": [
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class LanguageModel(object):\n",
        "    def __init__(self, trainCorpus):\n",
        "        '''\n",
        "        Initialize and train the model (i.e. estimate the model's underlying probability\n",
        "        distribution from the training corpus.)\n",
        "        '''\n",
        "\n",
        "        return\n",
        "\n",
        "    def generateSentence(self):\n",
        "        '''\n",
        "        Generate a sentence by drawing words according to the model's probability distribution.\n",
        "        Note: Think about how to set the length of the sentence in a principled way.\n",
        "        '''\n",
        "\n",
        "        raise NotImplementedError(\"Implement generateSentence in each subclass.\")\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        '''\n",
        "        Calculate the log probability of the sentence provided.\n",
        "        '''\n",
        "\n",
        "        raise NotImplementedError(\"Implement getSentenceProbability in each subclass.\")\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        '''\n",
        "        Calculate the perplexity of the corpus provided.\n",
        "        '''\n",
        "\n",
        "        raise NotImplementedError(\"Implement getCorpusPerplexity in each subclass.\")\n",
        "\n",
        "    def printSentences(self, n):\n",
        "        '''\n",
        "        Prints n sentences generated by your model.\n",
        "        '''\n",
        "\n",
        "        ### DO NOT EDIT ###\n",
        "        for i in range(n):\n",
        "            sent = self.generateSentence()\n",
        "            prob = self.getSentenceLogProbability(sent)\n",
        "            print('Log Probability:', prob , '\\tSentence:',sent)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf15l6f3etMV"
      },
      "source": [
        "## <font color='red'>TODO:</font> Unigram Model [18 points]\n",
        "\n",
        "Here, you will implement each of the 4 functions described above for an <b>unsmoothed unigram</b> model. The probability distribution of a word is given by $\\hat P(w)$.\n",
        "\n",
        "<font color='green'><b>Hints:</b></font>\n",
        "* <font color='green'>You should use a <b>dictionary</b> to map tokens to their unigram counts.</font>\n",
        "* <font color='green'>Since you never want to generate the start-of-sentence token `<s>`, you should <b>not</b> include it in your counts.</font>\n",
        "* <font color='green'>In general, avoid checking for membership in a list (i.e. avoid `x in lst`). Instead, use sets or dictionaries for this purpose $-$ membership checks are much faster on these data structures.</font>\n",
        "* <font color='green'>Do <b>not</b> modify the training or test corpora by using `.append(...)` or `.pop(...)` on them. This will cause unexpected behavior in the autograder tests, which do not expect you to be changing the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2uZdMsqeuf2"
      },
      "source": [
        "class UnigramModel(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        '''\n",
        "        Initialize and train an unsmoothed unigram language model.\n",
        "        We count the frequency of every token in trainCorpus except the start-of-sentence token.\n",
        "        '''\n",
        "        self.trainCorpus = trainCorpus\n",
        "        self.unigram_counts = {}  # maps word -> count\n",
        "        total = 0\n",
        "        for sentence in trainCorpus:\n",
        "            # Each sentence is a list of tokens with markers.\n",
        "            for token in sentence:\n",
        "                # Do not count the start-of-sentence token.\n",
        "                if token == START:\n",
        "                    continue\n",
        "                self.unigram_counts[token] = self.unigram_counts.get(token, 0) + 1\n",
        "                total += 1\n",
        "        self.total_count = total\n",
        "\n",
        "    def generateSentence(self):\n",
        "        '''\n",
        "        Generate a sentence by sampling words according to the unigram probabilities.\n",
        "        The sentence always begins with the start token `<s>` and generation stops when `</s>` is sampled.\n",
        "        '''\n",
        "        sentence = [START]\n",
        "        # Precompute the list of tokens (keys) and their corresponding weights.\n",
        "        # Note: Our unigram_counts does not include START.\n",
        "        tokens = list(self.unigram_counts.keys())\n",
        "        weights = [self.unigram_counts[t] for t in tokens]\n",
        "        while True:\n",
        "            word = random.choices(tokens, weights=weights, k=1)[0]\n",
        "            sentence.append(word)\n",
        "            if word == END:\n",
        "                break\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        '''\n",
        "        Calculate the log probability of a sentence (which is a list of tokens).\n",
        "        We ignore the start token `<s>` and compute:\n",
        "\n",
        "            log P(sentence) = sum_{token in sentence, token != <s>} log (count(token) / total_count)\n",
        "        '''\n",
        "        log_prob = 0.0\n",
        "        for token in sentence:\n",
        "            if token == START:\n",
        "                continue\n",
        "            count = self.unigram_counts.get(token, 0)\n",
        "            if count == 0:\n",
        "                # If a token was never seen in training, return -infinity.\n",
        "                return float('-inf')\n",
        "            prob = count / self.total_count\n",
        "            log_prob += math.log(prob)\n",
        "        return log_prob\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        '''\n",
        "        Calculate the perplexity of testCorpus. For each sentence in the corpus,\n",
        "        we sum the log probabilities for each token (ignoring `<s>`), and then compute:\n",
        "\n",
        "            Perplexity = exp( - (1/N) * sum(log P(token)) )\n",
        "\n",
        "        where N is the total number of tokens (excluding `<s>`) in the corpus.\n",
        "        '''\n",
        "        total_log_prob = 0.0\n",
        "        total_tokens = 0\n",
        "        for sentence in testCorpus:\n",
        "            for token in sentence:\n",
        "                if token == START:\n",
        "                    continue\n",
        "                count = self.unigram_counts.get(token, 0)\n",
        "                if count == 0:\n",
        "                    # If token never seen, return infinity perplexity.\n",
        "                    return float('inf')\n",
        "                prob = count / self.total_count\n",
        "                total_log_prob += math.log(prob)\n",
        "                total_tokens += 1\n",
        "        # Average log probability per token.\n",
        "        avg_log_prob = total_log_prob / total_tokens\n",
        "        perplexity = math.exp(-avg_log_prob)\n",
        "        return perplexity"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c6zfDsT-GrU"
      },
      "source": [
        "We provide you with a testing function that uses very simple training & test corpora (you could compute probability/perplexity by hand if you wanted to). This is just a <b>sanity check</b> $-$ passing this test does not guarantee you a perfect score in the autograder; this is simply to help you debug your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8uRRgLi6IVt",
        "outputId": "c60be0c0-e94d-4d45-d861-3059e3395948",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def sanityCheck(model_type):\n",
        "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
        "\n",
        "    #\tRead in the test corpus\n",
        "    train_corpus = [\"By the Late Classic , a network of few <unk> ( few <unk> ) linked various parts of the city , running for several kilometres through its urban core .\",\n",
        "    \"Few people realize how difficult it was to create Sonic 's graphics engine , which allowed for the incredible rate of speed the game 's known for .\"]\n",
        "    test_corpus = [\"Classic few parts of the game allowed for few <unk> <unk> incredible city .\",\n",
        "                   \"Few <unk> realize the difficult network , which linked the game to Sonic .\"]\n",
        "    train_corpus, _ = preprocess(train_corpus)\n",
        "    test_corpus, _ = preprocess(test_corpus)\n",
        "    sentence = preprocess([\"Sonic was difficult .\"])[0][0]\n",
        "\n",
        "    # These are the correct answers (don't change them!)\n",
        "    if model_type == \"unigram\":\n",
        "       senprobs = [-19.08542845, -114.5001481799, -108.7963657053, -53.6727664115, -55.4645258807]\n",
        "       trainPerp, testPerp = 41.3308239726, 38.0122981569\n",
        "       model = UnigramModel(train_corpus)\n",
        "    elif model_type == \"smoothed-unigram\":\n",
        "       senprobs = [-19.0405293515, -115.3479413049, -108.9114348746, -54.8190029616, -55.8122547346]\n",
        "       trainPerp, testPerp = 41.9994393615, 39.9531928383\n",
        "       model = SmoothedUnigramModel(train_corpus)\n",
        "    elif model_type == \"bigram\":\n",
        "       senprobs = [-float('inf'), -10.3450917073, -9.2464794186, -float('inf'), -float('inf')]\n",
        "       trainPerp, testPerp = 1.3861445461, float('inf')\n",
        "       model = BigramModel(train_corpus)\n",
        "    elif model_type == \"smoothed-bigram\":\n",
        "       senprobs = [-16.355820202, -76.0026113319, -74.2346475108, -47.2885760372, -51.2730261907]\n",
        "       trainPerp, testPerp = 12.2307627397, 26.7193157699\n",
        "       model = SmoothedBigramModelAD(train_corpus)\n",
        "    else: assert False, 'Invalid model_type'\n",
        "\n",
        "    print(\"--- TEST: generateSentence() ---\")\n",
        "    modelSen = model.generateSentence()\n",
        "    senTestPassed = isinstance(modelSen, list) and len(modelSen) > 1 and isinstance(modelSen[0], str)\n",
        "    if senTestPassed:\n",
        "        print (\"Test generateSentence() passed!\")\n",
        "    else:\n",
        "        print (\"Test generateSentence() failed; did not return a list of strings...\")\n",
        "\n",
        "    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n",
        "    sentences = [sentence, *train_corpus, *test_corpus]\n",
        "    failed = 0\n",
        "    for i in range(len(sentences)):\n",
        "        sen, correct_prob = sentences[i], senprobs[i]\n",
        "        prob = round(model.getSentenceLogProbability(sen), 10)\n",
        "        print(\"Correct log prob.:\", correct_prob, '\\tYour log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n",
        "        if prob != correct_prob: failed+=1\n",
        "\n",
        "    if not failed:\n",
        "        print (\"Test getSentenceProbability(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n",
        "\n",
        "    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n",
        "    train_perp = round(model.getCorpusPerplexity(train_corpus), 10)\n",
        "    test_perp = round(model.getCorpusPerplexity(test_corpus), 10)\n",
        "\n",
        "    print(\"Correct train perp.:\", trainPerp, '\\tYour train perp.:', train_perp, '\\t', 'PASSED' if trainPerp == train_perp else 'FAILED')\n",
        "    print(\"Correct test perp.:\", testPerp, '\\tYour test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n",
        "    train_passed, test_passed = train_perp == trainPerp, test_perp == testPerp\n",
        "    if train_passed and test_passed:\n",
        "        print(\"Test getCorpusPerplexity(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getCorpusPerplexity(...) failed on\", \"the training corpus and the testing corpus...\" if not train_passed and not test_passed else \"the testing corpus...\" if not test_passed else \"the training corpus...\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    sanityCheck('unigram')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -19.08542845 \tYour log prob.: -19.08542845 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -114.5001481799 \tYour log prob.: -114.5001481799 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -108.7963657053 \tYour log prob.: -108.7963657053 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -53.6727664115 \tYour log prob.: -53.6727664115 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -55.4645258807 \tYour log prob.: -55.4645258807 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 41.3308239726 \tYour train perp.: 41.3308239726 \t PASSED\n",
            "Correct test perp.: 38.0122981569 \tYour test perp.: 38.0122981569 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z8h9U63AkiG"
      },
      "source": [
        "Next, we provide you with another <b>sanity check</b> that trains your model on the *entire* training set, and tests your functions on a small corpus (10 sentences) of *real* test data.\n",
        "\n",
        "If your code is inefficient, you will likely see that this cell is taking too long. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sanityCheckFullDataset(model_type):\n",
        "    model = UnigramModel(train_dataset)\n",
        "    idxes = list(range(75,7500, 800))\n",
        "    small_test_corpus = [test_dataset[idx] for idx in idxes]\n",
        "    if model_type == 'unigram':\n",
        "        senprobs = [-80.7782190984, -174.4769654449, -136.455148267, -225.5890741503, -719.0142129846, -236.350443633, -126.0056604204, -47.3424655612, -47.7775372096, -138.8159941929]\n",
        "        testPerp = 881.0132848704\n",
        "        model = UnigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-unigram':\n",
        "        senprobs = [-80.8423009715, -174.5131424172, -136.3181234818, -225.357454098, -719.1543898871, -236.6682968913, -126.1965419509, -47.4369338195, -47.7692144935, -138.542462715]\n",
        "        testPerp = 881.6105352831\n",
        "        model = SmoothedUnigramModel(train_dataset)\n",
        "    elif model_type == 'bigram':\n",
        "        senprobs = [-float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -32.1502020637, -float('inf'), -float('inf')]\n",
        "        testPerp = float ('inf')\n",
        "        model = BigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-bigram':\n",
        "        senprobs = [-61.3754065648, -141.9754903887, -107.0849366076, -168.4944718788, -619.9409055374, -195.8159911677, -86.3762008156, -32.4764801981, -48.124714509, -124.687107856]\n",
        "        testPerp = 261.4247123506\n",
        "        model = SmoothedBigramModelAD(train_dataset)\n",
        "    else: assert False, 'Invalid model_type'\n",
        "    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n",
        "    failed = 0\n",
        "    for i in range(len(small_test_corpus)):\n",
        "        sen, correct_prob = small_test_corpus[i], senprobs[i]\n",
        "        prob = round(model.getSentenceLogProbability(sen), 10)\n",
        "        print(\"Correct log prob.:\", correct_prob, '\\tYour log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n",
        "        if prob != correct_prob: failed+=1\n",
        "\n",
        "    if not failed:\n",
        "        print (\"Test getSentenceProbability(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n",
        "\n",
        "    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n",
        "    test_perp = round(model.getCorpusPerplexity(small_test_corpus), 10)\n",
        "\n",
        "    print(\"Correct test perp.:\", testPerp, '\\tYour test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n",
        "    test_passed = test_perp == testPerp\n",
        "    if test_passed:\n",
        "        print(\"Test getCorpusPerplexity(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getCorpusPerplexity(...) failed on the testing corpus...\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('unigram')"
      ],
      "metadata": {
        "id": "_mCa6zasN-0j",
        "outputId": "06ef2e8e-65f0-4f63-bcef-89e95225ac7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -80.7782190984 \tYour log prob.: -80.7782190984 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -174.4769654449 \tYour log prob.: -174.4769654449 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -136.455148267 \tYour log prob.: -136.455148267 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -225.5890741503 \tYour log prob.: -225.5890741503 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -719.0142129846 \tYour log prob.: -719.0142129846 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -236.350443633 \tYour log prob.: -236.350443633 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -126.0056604204 \tYour log prob.: -126.0056604204 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -47.3424655612 \tYour log prob.: -47.3424655612 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -47.7775372096 \tYour log prob.: -47.7775372096 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -138.8159941929 \tYour log prob.: -138.8159941929 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: 881.0132848704 \tYour test perp.: 881.0132848704 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can train your model on the full WikiText corpus, and evaluate it on the held-out test set."
      ],
      "metadata": {
        "id": "lYJ0x5KpRrtU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1XHIg0xrUIt",
        "outputId": "13d20fb3-7357-4e9e-c80c-09f65a430d54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def runModel(model_type):\n",
        "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
        "    # Read the corpora\n",
        "    if model_type == 'unigram':\n",
        "        model = UnigramModel(train_dataset)\n",
        "    elif model_type == 'bigram':\n",
        "        model = BigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-unigram':\n",
        "        model = SmoothedUnigramModel(train_dataset)\n",
        "    else:\n",
        "        model = SmoothedBigramModelAD(train_dataset)\n",
        "\n",
        "    print(\"--------- 5 sentences from your model ---------\")\n",
        "    model.printSentences(5)\n",
        "\n",
        "    print (\"\\n--------- Corpus Perplexities ---------\")\n",
        "    print (\"Training Set:\", model.getCorpusPerplexity(train_dataset))\n",
        "    print (\"Testing Set:\", model.getCorpusPerplexity(test_dataset))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    runModel('unigram')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -23.92416647130524 \tSentence: ['<s>', 'statement', 'and', 'of', ',', '</s>']\n",
            "Log Probability: -519.9081045329366 \tSentence: ['<s>', 'half', 'video', 'species', 'accommodate', '.', 'career', 'junior', 'of', 'appealing', 'the', '.', 'an', 'to', 'some', 'telecommunications', 'of', 'dialogue', ',', 'and', 'the', 'damaged', 'a', 'mole', 'of', 'or', 'of', '.', 'agreed', 'in', 'rock', 'venomous', 'children', \"'Shara\", 'excess', 'A.', 'October', 'Commodore', 'Empire', 'of', 'big', 'February', 'season', 'the', 'in', 'Eva', 'say', ')', 'noisy', '(', 'had', 'states', 'of', 'within', 'season', '.', 'though', 'König', 'run', 'Males', '@-@', 'When', 'Sister', 'by', 'This', 'Company', 'with', 'from', 'the', 'also', 'in', 'these', '2014', '</s>']\n",
            "Log Probability: -20.719307699251726 \tSentence: ['<s>', 'the', 'Rangers', '\"', '</s>']\n",
            "Log Probability: -12.277194351857467 \tSentence: ['<s>', 'love', '</s>']\n",
            "Log Probability: -94.30513637598256 \tSentence: ['<s>', 'cave', '\"', 'The', 'popularity', ',', 'late', 'received', 'he', 'into', 'polemical', ',', ',', 'principal', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 1101.9435880270637\n",
            "Testing Set: 912.1574385914788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bGyA8vOfvRj"
      },
      "source": [
        "## <font color='red'>TODO:</font> Smoothed Unigram Model [18 points]\n",
        "\n",
        "Here, you will implement each of the 4 functions described above for a <b>unigram</b> model with <b>Laplace (add-one) smoothing</b>. The probability distribution of a word is given by $P_L(w)$. This type of smoothing takes away some of the probability mass for observed events and assigns it to unseen events.\n",
        "\n",
        "In order to smooth your model, you will need the number of words in the corpus, $N$, and the number of word types, $S$. The distinction between these is meaningful: $N$ indicates the number of word instances, where $S$ refers to the size of our vocabulary. For example, the sentence <em>the cat saw the dog</em> has four word types (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>dog</em>), but five word tokens (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>the</em>, <em>dog</em>). The token <em>the</em> appears twice in the sentence, but they share the same type <em>the</em>.\n",
        "\n",
        "If $c(w)$ is the frequency of $w$ in the training data, you can compute $P_L(w)$ as follows:\n",
        "\n",
        "$$P_L(w)=\\frac{c(w)+1}{N+S}$$\n",
        "\n",
        "<font color='green'><b>Hints:</b></font>\n",
        "* <font color='green'>You may find it convenient to make your `SmoothedUnigramModel` inherit your `UnigramModel`, and then override the function(s) that need to be changed.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzX-UZJPfvRn"
      },
      "source": [
        "class SmoothedUnigramModel(UnigramModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        '''\n",
        "        Initialize and train a Laplace smoothed unigram language model.\n",
        "        This uses the same counts as in the unsmoothed model but computes probabilities\n",
        "        with add-one smoothing:\n",
        "\n",
        "            P_L(w) = (c(w) + 1) / (N + S)\n",
        "\n",
        "        where N is the total count of tokens (excluding the START token) and S is the number\n",
        "        of distinct tokens.\n",
        "        '''\n",
        "        # First, initialize using the unsmoothed unigram model\n",
        "        super().__init__(trainCorpus)\n",
        "        # Set S as the vocabulary size (of tokens counted)\n",
        "        self.vocab_size = len(self.unigram_counts)\n",
        "        # N is the total token count computed in __init__\n",
        "        # Precompute the smoothing denominator:\n",
        "        self.smoothing_denominator = self.total_count + self.vocab_size\n",
        "\n",
        "    def generateSentence(self):\n",
        "        '''\n",
        "        Generate a sentence by sampling words from the smoothed unigram distribution.\n",
        "        The sentence always begins with `<s>` (START) and continues until the end token `</s>` is generated.\n",
        "        For each token, its sampling weight is (c(token) + 1).\n",
        "        '''\n",
        "        sentence = [START]\n",
        "        # Get the list of tokens (the keys of our count dictionary)\n",
        "        tokens = list(self.unigram_counts.keys())\n",
        "        # Compute weights as (count + 1) for each token.\n",
        "        weights = [self.unigram_counts[t] + 1 for t in tokens]\n",
        "        while True:\n",
        "            # Sample one token according to these weights.\n",
        "            word = random.choices(tokens, weights=weights, k=1)[0]\n",
        "            sentence.append(word)\n",
        "            if word == END:\n",
        "                break\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        '''\n",
        "        Calculate the log probability of a sentence (a list of tokens) under the smoothed unigram model.\n",
        "        We ignore the start-of-sentence token `<s>` and, for every other token, use:\n",
        "\n",
        "            log P(w) = log((c(w)+1) / (N+S))\n",
        "\n",
        "        If a token was unseen in training, c(w)=0 and it still gets a probability of 1/(N+S).\n",
        "        '''\n",
        "        log_prob = 0.0\n",
        "        for token in sentence:\n",
        "            if token == START:\n",
        "                continue\n",
        "            count = self.unigram_counts.get(token, 0)\n",
        "            prob = (count + 1) / self.smoothing_denominator\n",
        "            log_prob += math.log(prob)\n",
        "        return log_prob\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        '''\n",
        "        Calculate the perplexity of testCorpus under the smoothed unigram model.\n",
        "        For each token (ignoring `<s>`), we sum the log probabilities.\n",
        "\n",
        "        Let N be the total number of tokens (excluding `<s>`) in the corpus.\n",
        "        Then:\n",
        "\n",
        "            Perplexity = exp( - (1/N) * sum(log P(w)) )\n",
        "        '''\n",
        "        total_log_prob = 0.0\n",
        "        total_tokens = 0\n",
        "        for sentence in testCorpus:\n",
        "            for token in sentence:\n",
        "                if token == START:\n",
        "                    continue\n",
        "                count = self.unigram_counts.get(token, 0)\n",
        "                prob = (count + 1) / self.smoothing_denominator\n",
        "                total_log_prob += math.log(prob)\n",
        "                total_tokens += 1\n",
        "        avg_log_prob = total_log_prob / total_tokens\n",
        "        perplexity = math.exp(-avg_log_prob)\n",
        "        return perplexity"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBy9QaEdCUbc",
        "outputId": "7e9cd834-e14e-4622-f76c-c881c07ac60f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheck('smoothed-unigram')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -19.0405293515 \tYour log prob.: -19.0405293515 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -115.3479413049 \tYour log prob.: -115.3479413049 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -108.9114348746 \tYour log prob.: -108.9114348746 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -54.8190029616 \tYour log prob.: -54.8190029616 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -55.8122547346 \tYour log prob.: -55.8122547346 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 41.9994393615 \tYour train perp.: 41.9994393615 \t PASSED\n",
            "Correct test perp.: 39.9531928383 \tYour test perp.: 39.9531928383 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the next sanity check trains your model on the *entire* training set, you will likely see that it is taking too long if you have inefficiences in your code. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."
      ],
      "metadata": {
        "id": "pxeKnWk1TTfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('smoothed-unigram')"
      ],
      "metadata": {
        "id": "210t2j1GS1Xd",
        "outputId": "e8c6a0b4-26cc-4198-fd59-869c1ca9a9df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -80.8423009715 \tYour log prob.: -80.8423009715 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -174.5131424172 \tYour log prob.: -174.5131424172 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -136.3181234818 \tYour log prob.: -136.3181234818 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -225.357454098 \tYour log prob.: -225.357454098 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -719.1543898871 \tYour log prob.: -719.1543898871 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -236.6682968913 \tYour log prob.: -236.6682968913 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -126.1965419509 \tYour log prob.: -126.1965419509 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -47.4369338195 \tYour log prob.: -47.4369338195 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -47.7692144935 \tYour log prob.: -47.7692144935 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -138.542462715 \tYour log prob.: -138.542462715 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: 881.6105352831 \tYour test perp.: 881.6105352831 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6agWmpjdCWOt",
        "outputId": "8ca2bdee-2d6c-4d87-f355-19f6410d656a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('smoothed-unigram')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -333.6486800431614 \tSentence: ['<s>', 'CD', 'the', 'including', 'cooler', 'cars', 'stated', 'the', ',', 'multiple', '.', 'they', '<UNK>', 'first', 'and', '.', 'Through', 'around', '3', 'from', 'of', 'as', '@-@', 'Sandy', 'a', 'in', 'and', 'paths', 'Since', 'apparently', 'treats', 'arrangement', 'restricted', 'effect', 'leg', 'at', 'said', 'in', 'becoming', 'Michael', '%', 'out', 'Park', 'of', 'on', 'Ico', 'was', '</s>']\n",
            "Log Probability: -7.320521604431183 \tSentence: ['<s>', 'to', '</s>']\n",
            "Log Probability: -3.3374709753302536 \tSentence: ['<s>', '</s>']\n",
            "Log Probability: -110.49077449690505 \tSentence: ['<s>', 'US', ',', '36', 'titles', '<UNK>', 'bonus', 'the', 'of', '.', 'and', 'of', 'the', 'time', 'writer', 'training', 'of', 'transfusions', '</s>']\n",
            "Log Probability: -231.6529540235593 \tSentence: ['<s>', 'leagues', 'Price', 'anonymously', '°', 'way', 'the', 'the', 'Now', 'procured', 'season', 'remainder', 'the', 'of', '.', 'bowls', 'and', 'the', 'off', 'Emperor', ',', 'War', 'possible', '.', '.', 'South', 'for', 'low', 'may', 'on', 'has', \"'s\", '1911', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 1103.0243317913958\n",
            "Testing Set: 914.4724502277719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGtcWVMGiEGw"
      },
      "source": [
        "## <font color='red'>TODO:</font> Bigram Model [18 points]\n",
        "\n",
        "Here, you will implement each of the 4 functions described above for an <b>unsmoothed bigram</b> model. The probability distribution of a word is given by $\\hat P(w'|w)$. Thus, the probability of $w_i$ is conditioned on $w_{i-1}$.\n",
        "\n",
        "<font color='green'><b>Hints:</b></font>\n",
        "* <font color='green'>You should use a dictionary of dictionaries to store your bigram counts. That is, the outer dictionary should map $w$ to another dictionary that maps $w'$ to the number of times $w'$ occurs after $w$.</font>\n",
        "* <font color='green'>Do <b>not</b> attempt to iterate over all possible bigrams in your voabulary: <em>only store bigrams that actually occur in your training data.</em> You will run into timeout or out-of-memory issues if you attempt to enumerate all bigrams.</font>\n",
        "* <font color='green'>Similarly, avoid nested loops over the training data.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ojk_q0YiEGx"
      },
      "source": [
        "class BigramModel(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        \"\"\"\n",
        "        Initialize and train an unsmoothed bigram language model.\n",
        "        Computes:\n",
        "          - self.bigram_counts: a dict mapping a word w (as a conditioning token)\n",
        "              to a dict mapping a following word w' to the count of (w, w') occurrences.\n",
        "          - self.unigram_counts: a dict mapping each word (used as a conditioning word)\n",
        "              to the total count of its occurrences as the first element in a bigram.\n",
        "          - self.vocab: the set of all tokens that ever occur as a conditioning word.\n",
        "\n",
        "        Note: We iterate over each sentence (which is assumed to be a list of tokens with\n",
        "        START and END markers) and count each adjacent pair.\n",
        "        \"\"\"\n",
        "        self.bigram_counts = {}\n",
        "        self.unigram_counts = {}\n",
        "        for sentence in trainCorpus:\n",
        "            # Iterate over each bigram in the sentence.\n",
        "            for i in range(len(sentence) - 1):\n",
        "                w = sentence[i]\n",
        "                w_next = sentence[i + 1]\n",
        "                if w not in self.bigram_counts:\n",
        "                    self.bigram_counts[w] = {}\n",
        "                self.bigram_counts[w][w_next] = self.bigram_counts[w].get(w_next, 0) + 1\n",
        "                self.unigram_counts[w] = self.unigram_counts.get(w, 0) + 1\n",
        "        # For convenience, store the vocabulary (conditioning words)\n",
        "        self.vocab = set(self.unigram_counts.keys())\n",
        "\n",
        "    def generateSentence(self):\n",
        "        \"\"\"\n",
        "        Generate a sentence by sampling words according to the bigram distribution.\n",
        "        Always start with the START token and sample until the END token is generated.\n",
        "        At each step, sample the next word from P(w' | current) defined as:\n",
        "          count(current, w') / count(current)\n",
        "        \"\"\"\n",
        "        sentence = [START]\n",
        "        current = START\n",
        "        while True:\n",
        "            # If current is not seen as a conditioning word, break (should not happen ideally).\n",
        "            if current not in self.bigram_counts:\n",
        "                break\n",
        "            next_dict = self.bigram_counts[current]\n",
        "            words = list(next_dict.keys())\n",
        "            counts = list(next_dict.values())\n",
        "            # Sample one word using the counts as weights.\n",
        "            next_word = random.choices(words, weights=counts, k=1)[0]\n",
        "            sentence.append(next_word)\n",
        "            if next_word == END:\n",
        "                break\n",
        "            current = next_word\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        \"\"\"\n",
        "        Compute the log probability of a sentence (list of tokens) under the unsmoothed bigram model.\n",
        "        For a sentence [w0, w1, ..., wN] (with w0=START, wN=END), the probability is:\n",
        "          ∏_{i=1}^N P(w_i | w_{i-1}) = ∏_{i=1}^N ( count(w_{i-1}, w_i) / count(w_{i-1}) )\n",
        "        We sum the logs for numerical stability.\n",
        "        If any bigram is unseen, return -infinity.\n",
        "        \"\"\"\n",
        "        log_prob = 0.0\n",
        "        for i in range(1, len(sentence)):\n",
        "            prev = sentence[i - 1]\n",
        "            curr = sentence[i]\n",
        "            if prev not in self.bigram_counts or curr not in self.bigram_counts[prev]:\n",
        "                return float(\"-inf\")\n",
        "            count_bigram = self.bigram_counts[prev][curr]\n",
        "            count_prev = self.unigram_counts[prev]\n",
        "            prob = count_bigram / count_prev\n",
        "            log_prob += math.log(prob)\n",
        "        return log_prob\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        \"\"\"\n",
        "        Compute the perplexity of the test corpus under the unsmoothed bigram model.\n",
        "        For each sentence, compute its log probability (excluding the START token).\n",
        "        Let N be the total number of tokens (excluding START) in the corpus.\n",
        "        Then perplexity is defined as:\n",
        "            Perplexity = exp( - (1/N) * (sum_{all tokens} log P(token | previous_token)) )\n",
        "        If any sentence has a probability of 0 (i.e. log probability -inf), return infinity.\n",
        "        \"\"\"\n",
        "        total_log_prob = 0.0\n",
        "        total_tokens = 0\n",
        "        for sentence in testCorpus:\n",
        "            # We assume sentence is a list of tokens; ignore the START token for counting.\n",
        "            lp = self.getSentenceLogProbability(sentence)\n",
        "            if lp == float(\"-inf\"):\n",
        "                return float(\"inf\")\n",
        "            total_log_prob += lp\n",
        "            # Count all tokens except the first (START)\n",
        "            total_tokens += (len(sentence) - 1)\n",
        "        avg_log_prob = total_log_prob / total_tokens\n",
        "        perplexity = math.exp(-avg_log_prob)\n",
        "        return perplexity"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J8O8mCICZO5",
        "outputId": "8ac6f69b-ab75-497a-eb7a-4dbb36674432",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheck('bigram')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -10.3450917073 \tYour log prob.: -10.3450917073 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -9.2464794186 \tYour log prob.: -9.2464794186 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 1.3861445461 \tYour train perp.: 1.3861445461 \t PASSED\n",
            "Correct test perp.: inf \tYour test perp.: inf \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the next sanity check trains your model on the *entire* training set, you will likely see that it is taking too long if you have inefficiences in your code. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."
      ],
      "metadata": {
        "id": "IMcAZYrpUE0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('bigram')"
      ],
      "metadata": {
        "id": "DIolz_8sS3B9",
        "outputId": "95746287-2303-4ea2-9ad8-354953a533dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -32.1502020637 \tYour log prob.: -32.1502020637 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: inf \tYour test perp.: inf \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKop-qYKCZO5",
        "outputId": "3de23d96-004c-4b26-fea3-4de986b5c129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('bigram')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -223.7288770552257 \tSentence: ['<s>', 'Leo', 'II', 'in', 'place', 'in', '1983', ')', 'and', '1838', ',', 'stating', 'in', '1844', 'without', 'this', 'redevelopment', 'committee', ',', 'then', 'the', 'bold', 'series', ',', 'as', 'a', 'shrubby', '<UNK>', 'with', 'the', 'UK', ',', 'Freshman', 'All', 'the', 'ship', 'broke', 'the', 'same', 'as', 'a', 'violation', 'of', 'the', 'son', 'of', 'the', 'Beverly', 'Crusher', '.', '</s>']\n",
            "Log Probability: -28.86800245100175 \tSentence: ['<s>', '<UNK>', ',', 'before', 'he', 'still', 'taxiing', '.', '</s>']\n",
            "Log Probability: -10.863760236655747 \tSentence: ['<s>', 'Under', '.', '</s>']\n",
            "Log Probability: -12.55974195903803 \tSentence: ['<s>', 'Empire', '.', '</s>']\n",
            "Log Probability: -131.3944907845666 \tSentence: ['<s>', 'The', 'Fur', 'Company', ',', 'scoring', 'a', 'distance', 'before', 'making', 'it', 'was', 'the', 'song', 'of', 'a', 'glimpse', 'of', 'the', 'neck', 'to', '320', 'km', ')', 'was', 'recorded', 'by', 'disability', 'and', 'the', 'mycelia', '.', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 76.92394608735728\n",
            "Testing Set: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPBeyKUsfnnW"
      },
      "source": [
        "## <font color='red'>TODO:</font> Smoothed Bigram Model [18 points]\n",
        "\n",
        "Here, you will implement each of the 4 functions described above for a <b>bigram</b> model with <b>absolute discounting</b>. The probability distribution of a word is given by $P_{AD}(w’|w)$.\n",
        "\n",
        "In order to smooth your model, you need to compute a discounting factor $D$. If $n_k$ is the number of bigrams $w_1w_2$ that appear exactly $k$ times, you can compute $D$ as:\n",
        "\n",
        "$$D=\\frac{n_1}{n_1+2n_2}$$\n",
        "\n",
        "For each word $w$, you then need to compute the number of bigram types $ww’$ as follows:\n",
        "\n",
        "$$S(w)=|\\{w’\\mid c(ww’)>0\\}|$$\n",
        "\n",
        "where $c(ww’)$ is the frequency of $ww’$ in the training data. In other words, $S(w)$ is the number of unique words that follow $w$ at least once in the training data.\n",
        "\n",
        "Finally, you can compute $P_{AD}(w’|w)$ as follows:\n",
        "\n",
        "$$P_{AD}(w’|w)=\\frac{\\max \\big (c(ww’)-D,0\\big )}{c(w)}+\\bigg (\\frac{D}{c(w)}\\cdot S(w) \\cdot P_L(w’)\\bigg )$$\n",
        "\n",
        "where $c(w)$ is the frequency of $w$ in the training data and $P_L(w’)$ is the Laplace-smoothed unigram probability of $w’$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1klb00wtVtS"
      },
      "source": [
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class SmoothedBigramModelAD(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        ### TODO ###\n",
        "        # 1) Build bigram counts and single-word counts.\n",
        "        self.bigramCounts = defaultdict(lambda: defaultdict(int))\n",
        "        self.wordCounts   = defaultdict(int)\n",
        "\n",
        "        # For each sentence in the training corpus:\n",
        "        for sentence in trainCorpus:\n",
        "            # e.g. sentence = [<s>, w1, w2, ..., </s>]\n",
        "            for i in range(len(sentence) - 1):\n",
        "                w1 = sentence[i]\n",
        "                w2 = sentence[i+1]\n",
        "                self.bigramCounts[w1][w2] += 1\n",
        "                self.wordCounts[w1]       += 1\n",
        "            # Count the last token as well (often optional, but consistent):\n",
        "            self.wordCounts[sentence[-1]] += 1\n",
        "\n",
        "        # 2) Count n1 and n2 for discounting\n",
        "        n1 = 0\n",
        "        n2 = 0\n",
        "        for w1 in self.bigramCounts:\n",
        "            for w2, count in self.bigramCounts[w1].items():\n",
        "                if count == 1:\n",
        "                    n1 += 1\n",
        "                elif count == 2:\n",
        "                    n2 += 1\n",
        "\n",
        "        # 3) Compute discount D\n",
        "        if (n1 + 2*n2) == 0:\n",
        "            self.D = 0.0\n",
        "        else:\n",
        "            self.D = float(n1) / float(n1 + 2*n2)\n",
        "\n",
        "        # 4) Store S(w) = number of distinct next-words after w\n",
        "        self.successors = {}\n",
        "        for w1 in self.bigramCounts:\n",
        "            self.successors[w1] = len(self.bigramCounts[w1])\n",
        "\n",
        "        # 5) Build a Laplace-unigram fallback model\n",
        "        #    We'll define it here in-line or you can reference a separate class\n",
        "        self.uniCounts     = defaultdict(int)\n",
        "        self.totalUnigrams = 0\n",
        "        self.vocabSet      = set()\n",
        "\n",
        "        for sentence in trainCorpus:\n",
        "            for w in sentence:\n",
        "                # often skip <s> if you don't want it in the generation\n",
        "                if w == START:\n",
        "                    continue\n",
        "                self.uniCounts[w] += 1\n",
        "                self.totalUnigrams += 1\n",
        "                self.vocabSet.add(w)\n",
        "\n",
        "        self.vocabSize = len(self.vocabSet)\n",
        "\n",
        "        # For generation, build a set of valid tokens\n",
        "        # (some might remove <s> so it won't appear mid-sentence)\n",
        "        self.generationVocab = set(self.wordCounts.keys())\n",
        "        if START in self.generationVocab:\n",
        "            self.generationVocab.remove(START)\n",
        "\n",
        "\n",
        "    def laplaceUnigramProb(self, w):\n",
        "        \"\"\"\n",
        "        Helper: Laplace-smoothed probability of word w\n",
        "        P_L(w) = (count(w) + 1) / (totalUnigrams + vocabSize)\n",
        "        \"\"\"\n",
        "        return float(self.uniCounts[w] + 1) / float(self.totalUnigrams + self.vocabSize)\n",
        "\n",
        "\n",
        "    def getBigramADProbability(self, w1, w2):\n",
        "        \"\"\"\n",
        "        Absolute discounting bigram probability:\n",
        "          P_AD(w2|w1) = max(c(w1,w2)-D, 0)/c(w1) + (D/c(w1))*S(w1)*P_L(w2)\n",
        "        \"\"\"\n",
        "        c_w1   = self.wordCounts[w1]\n",
        "        c_w1w2 = self.bigramCounts[w1][w2] if w2 in self.bigramCounts[w1] else 0\n",
        "\n",
        "        # if w1 not observed, fallback to Laplace unigram\n",
        "        if c_w1 == 0:\n",
        "            return self.laplaceUnigramProb(w2)\n",
        "\n",
        "        main_part = max(c_w1w2 - self.D, 0.0) / float(c_w1)\n",
        "        leftover  = (self.D / float(c_w1)) * self.successors.get(w1, 0) * self.laplaceUnigramProb(w2)\n",
        "        return main_part + leftover\n",
        "\n",
        "\n",
        "    def generateSentence(self):\n",
        "        ### TODO ###\n",
        "        # Generate from the bigram distribution until we hit </s> or a length cap.\n",
        "        sentence = [START]\n",
        "        while True:\n",
        "            w1 = sentence[-1]\n",
        "            next_word = self.sampleNext(w1)\n",
        "            sentence.append(next_word)\n",
        "            if next_word == END or len(sentence) > 100:\n",
        "                break\n",
        "        return sentence\n",
        "\n",
        "\n",
        "    def sampleNext(self, w1):\n",
        "        \"\"\"\n",
        "        Helper for generateSentence:\n",
        "        1) For each w2 in generationVocab U {</s>}, get P_AD(w2|w1).\n",
        "        2) Sample from that distribution.\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "        total_p = 0.0\n",
        "        # union with END so we can actually generate the stop token\n",
        "        all_words = self.generationVocab.union({END})\n",
        "\n",
        "        for w2 in all_words:\n",
        "            p = self.getBigramADProbability(w1, w2)\n",
        "            candidates.append((w2, p))\n",
        "            total_p += p\n",
        "\n",
        "        # random draw\n",
        "        r = random.random()\n",
        "        cumulative = 0.0\n",
        "        for (word, prob) in candidates:\n",
        "            prob_norm = prob / (total_p + 1e-12)\n",
        "            cumulative += prob_norm\n",
        "            if r <= cumulative:\n",
        "                return word\n",
        "        return END  # fallback\n",
        "\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        ### TODO ###\n",
        "        # Sum log-probs for each bigram in [<s>, w1, w2, ..., </s>].\n",
        "        log_prob = 0.0\n",
        "        for i in range(1, len(sentence)):\n",
        "            w1 = sentence[i-1]\n",
        "            w2 = sentence[i]\n",
        "            p = self.getBigramADProbability(w1, w2)\n",
        "            if p <= 0.0:\n",
        "                return float('-inf')\n",
        "            log_prob += math.log(p)\n",
        "        return log_prob\n",
        "\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        ### TODO ###\n",
        "        # Perplexity = exp( - (1/N) * sum_of_log_probs ), N = total predicted words\n",
        "        totalLogProb = 0.0\n",
        "        totalWords   = 0\n",
        "\n",
        "        for sent in testCorpus:\n",
        "            # Each sentence is e.g. [<s>, w1, w2, ..., </s>]\n",
        "            # We'll compute log prob of that sentence\n",
        "            lp = self.getSentenceLogProbability(sent)\n",
        "            if lp == float('-inf'):\n",
        "                return float('inf')\n",
        "            totalLogProb += lp\n",
        "            # #words predicted excludes the first <s>, so len(sent)-1\n",
        "            totalWords += (len(sent) - 1)\n",
        "\n",
        "        if totalWords == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        avgLogProb = totalLogProb / float(totalWords)\n",
        "        perp = math.exp(-avgLogProb)\n",
        "        return perp\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BLplBkmtfWG",
        "outputId": "4a7fc58a-885f-4ebf-e605-df630bd4620d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheck('smoothed-bigram')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -16.355820202 \tYour log prob.: -16.355820202 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -76.0026113319 \tYour log prob.: -76.0026113319 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -74.2346475108 \tYour log prob.: -74.2346475108 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -47.2885760372 \tYour log prob.: -47.2885760372 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -51.2730261907 \tYour log prob.: -51.2730261907 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 12.2307627397 \tYour train perp.: 12.2307627397 \t PASSED\n",
            "Correct test perp.: 26.7193157699 \tYour test perp.: 26.7193157699 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the next sanity check trains your model on the *entire* training set, you will likely see that it is taking too long if you have inefficiences in your code. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."
      ],
      "metadata": {
        "id": "1dgNaW3HUGuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('smoothed-bigram')"
      ],
      "metadata": {
        "id": "85sJoiXsS4Zv",
        "outputId": "5eed2a61-8a0b-4e75-9b6d-84efd1382fdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -61.3754065648 \tYour log prob.: -61.3754065648 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -141.9754903887 \tYour log prob.: -141.9754903887 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -107.0849366076 \tYour log prob.: -107.0849366076 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -168.4944718788 \tYour log prob.: -168.4944718788 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -619.9409055374 \tYour log prob.: -619.9409055374 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -195.8159911677 \tYour log prob.: -195.8159911677 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -86.3762008156 \tYour log prob.: -86.3762008156 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -32.4764801981 \tYour log prob.: -32.4764801981 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -48.124714509 \tYour log prob.: -48.124714509 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -124.687107856 \tYour log prob.: -124.687107856 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: 261.4247123506 \tYour test perp.: 261.4247123506 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GibKGwdXtiUQ",
        "outputId": "285ab162-4b66-445b-c026-f2d1d49520df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('smoothed-bigram')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -343.41528478362153 \tSentence: ['<s>', 'In', 'addition', ',', 'committee', 'to', 'find', 'copyright', 'to', 'the', 'ship', 'to', 'the', 'cover', 'versions', 'of', 'death', 'in', '<UNK>', '<UNK>', '\"', 'featuring', 'Shimomura', 'has', 'a', 'colony', 'on', '38', '–', 'Simpson', 'hurricane', 'season', 'for', 'one', 'of', 'marked', 'by', 'the', 'rest', 'of', 'its', '<UNK>', \"'s\", 'arrangements', 'for', 'more', 'susceptible', 'to', '150', '<UNK>', ':', '30', 'miles', '(', '23', '–', '13', 'alive', \"'s\", 'of', 'Captain', 'Picard', 'helps', ',', 'was', 'later', 'the', 'active', 'altarpieces', '.', '</s>']\n",
            "Log Probability: -16.763485385718216 \tSentence: ['<s>', 'At', 'the', 'inspiration', '.', '</s>']\n",
            "Log Probability: -91.77632944315609 \tSentence: ['<s>', 'In', 'the', 'Lower', 'Tertiary', 'working', ',', 'one', 'of', 'Mariana', 'is', 'Board', 'Mothers', 'of', '<UNK>', 'a', '\"', '.', '\"', '</s>']\n",
            "Log Probability: -51.411988857652716 \tSentence: ['<s>', 'Dylan', 'and', ',', 'did', 'not', 'accept', '<UNK>', 'or', 'life', '\"', '</s>']\n",
            "Log Probability: -67.89284115220485 \tSentence: ['<s>', 'Luis', 'operas', 'in', 'any', 'had', 'a', '<UNK>', '<UNK>', ')', ',', 'and', 'southward', '.', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 98.5581292053259\n",
            "Testing Set: 272.57201979320354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtMMWXC0Emwq"
      },
      "source": [
        "## Food for Thought\n",
        "We provide you some questions to think about. <b>You do not need to answer these questions, but we encourage you to give them some thought.</b>\n",
        "<ol>\n",
        "<li>When generating sentences with the unigram model, what controls the length of the generated sentences? How does this differ from the sentences produced by the bigram models?\n",
        "<li>Consider the probability of the generated sentences according to your models. Do your models assign drastically different probabilities to the different sets of sentences? Why do you think that is?\n",
        "<li>Look back at the sentences generated using your models. In your opinion, which model produces better / more realistic sentences?\n",
        "<li>For each of the four models, which test corpus has the highest perplexity? Why?\n",
        "<li> Why do you think it might be a bad idea to use Laplace (add-one) smoothing for a bigram model? How does the absolute discounting method help?\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa3qZXUBePFW"
      },
      "source": [
        "# What to Submit\n",
        "\n",
        "To submit the assignment, download this notebook as a <TT>.py</TT> file. You can do this by going to <TT>File > Download > Download .py</TT>. Then submit it to the autograder in Gradescope. <b>Do not try to submit it as a <TT>.ipynb</TT> file!</b>\n",
        "\n",
        "**Reminder: Make sure that you access the Gradescope submission page via the corresponding assignment in Coursera!** Failure to do so may result in the inability to push your grades to Coursera. (The same goes for quizzes!)\n",
        "\n",
        "Note that it should take <b>less than 10 minutes</b> to see your score after you have submitted to Gradescope. If your submission runs significantly longer than that, you probably have inefficiency issues in your code!"
      ]
    }
  ]
}